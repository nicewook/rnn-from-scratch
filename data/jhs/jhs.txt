January 2017People who are powerful but uncharismatic will tend to be disliked.
Their power makes them a target for criticism that they don't have
the charisma to disarm. That was Hillary Clinton's problem. It also
tends to be a problem for any CEO who is more of a builder than a
schmoozer. And yet the builder-type CEO is (like Hillary) probably
the best person for the job.I don't think there is any solution to this problem. It's human
nature. The best we can do is to recognize that it's happening, and
to understand that being a magnet for criticism is sometimes a sign
not that someone is the wrong person for a job, but that they're
the right one.
January 2017Because biographies of famous scientists tend to 
edit out their mistakes, we underestimate the 
degree of risk they were willing to take.
And because anything a famous scientist did that
wasn't a mistake has probably now become the
conventional wisdom, those choices don't
seem risky either.Biographies of Newton, for example, understandably focus
more on physics than alchemy or theology.
The impression we get is that his unerring judgment
led him straight to truths no one else had noticed.
How to explain all the time he spent on alchemy
and theology?  Well, smart people are often kind of
crazy.But maybe there is a simpler explanation. Maybe
the smartness and the craziness were not as separate
as we think. Physics seems to us a promising thing
to work on, and alchemy and theology obvious wastes
of time. But that's because we know how things
turned out. In Newton's day the three problems 
seemed roughly equally promising. No one knew yet
what the payoff would be for inventing what we
now call physics; if they had, more people would 
have been working on it. And alchemy and theology
were still then in the category Marc Andreessen would 
describe as "huge, if true."Newton made three bets. One of them worked. But 
they were all risky.
November 2016If you're a California voter, there is an important proposition
on your ballot this year: Proposition 62, which bans the death
penalty.When I was younger I used to think the debate about the death
penalty was about when it's ok to take a human life.  Is it ok
to kill a killer?But that is not the issue here.The real world does not work like the version I was shown on TV growing up.  The police 
often arrest the wrong person.
Defendants' lawyers are often incompetent.  And prosecutors
are often motivated more by publicity than justice.In the real world,       
about 4% of people sentenced to death
are innocent.
So this is not about whether it's ok to kill killers. This
is about whether it's ok to kill innocent people.A child could answer that one for you.This year, in California, you have a chance to end this, by
voting yes on Proposition 62. But beware, because there is another 
proposition, Proposition 66, whose goal is to make it 
easier to execute people. So yes on 62, no on 66.It's time.
April 2016(This is a talk I gave at an event called Opt412 in Pittsburgh.
Much of it will apply to other towns.  But not all, because
as I say in the talk, Pittsburgh has some important advantages over
most would-be startup hubs.)What would it take to make Pittsburgh into a startup hub, like
Silicon Valley?  I feel like I understand Pittsburgh pretty well,
because I grew up here, in Monroeville. And I understand Silicon
Valley pretty well because that's where I live now.  Could you get
that kind of startup ecosystem going here?When I agreed to speak here, I didn't think I'd be able to give a
very optimistic talk.  I thought I'd be talking about what Pittsburgh
could do to become a startup hub, very much in the subjunctive.
Instead I'm going to talk about what Pittsburgh can do.What changed my mind was an article I read in, of all places, the New
York Times food section.  The title was "Pittsburgh's Youth-Driven
Food Boom."  To most people that might not even sound interesting,
let alone something related to startups.  But it was electrifying
to me to read that title.  I don't think I could pick a more promising
one if I tried.  And when I read the article I got even more excited.
It said "people ages 25 to 29 now make up 7.6 percent of all
residents, up from 7 percent about a decade ago."  Wow, I thought,
Pittsburgh could be the next Portland.  It could become the cool
place all the people in their twenties want to go live.When I got here a couple days ago, I could feel the difference.  I
lived here from 1968 to 1984.  I didn't realize it at the time, but
during that whole period the city was in free fall. On top of the
flight to the suburbs that happened everywhere, the steel and nuclear
businesses were both dying. Boy are things different now.  It's not
just that downtown seems a lot more prosperous. There is an energy
here that was not here when I was a kid.When I was a kid, this was a place young people left. Now it's a
place that attracts them.What does that have to do with startups?  Startups are made
of people, and the average age of the people in a typical startup
is right in that 25 to 29 bracket.I've seen how powerful it is for a city to have those people.  Five
years ago they shifted the center of gravity of Silicon Valley from
the peninsula to San Francisco.  Google and Facebook are on the
peninsula, but the next generation of big winners are all in SF.
The reason the center of gravity shifted was the talent war, for
programmers especially.  Most 25 to 29 year olds want to live in
the city, not down in the boring suburbs.  So whether they like it
or not, founders know they have to be in the city.  I know multiple
founders who would have preferred to live down in the Valley proper,
but who made themselves move to SF because they knew otherwise
they'd lose the talent war.So being a magnet for people in their twenties is a very promising
thing to be.  It's hard to imagine a place becoming a startup hub
without also being that.  When I read that statistic about the
increasing percentage of 25 to 29 year olds, I had exactly the same
feeling of excitement I get when I see a startup's graphs start to
creep upward off the x axis.Nationally the percentage of 25 to 29 year olds is 6.8%.  That means
you're .8% ahead.  The population is 306,000, so we're talking about
a surplus of about 2500 people.  That's the population of a small
town, and that's just the surplus. So you have a toehold.  Now you
just have to expand it.And though "youth-driven food boom" may sound frivolous, it is
anything but.  Restaurants and cafes are a big part of the personality
of a city.  Imagine walking down a street in Paris. What are you
walking past?  Little restaurants and cafes.  Imagine driving through
some depressing random exurb.  What are you driving past?  Starbucks
and McDonalds and Pizza Hut.   As Gertrude Stein said, there is no
there there. You could be anywhere.These independent restaurants and cafes are not just feeding people.
They're making there be a there here.So here is my first concrete recommendation for turning Pittsburgh
into the next Silicon Valley: do everything you can to encourage
this youth-driven food boom.  What could the city do?  Treat the
people starting these little restaurants and cafes as your users,
and go ask them what they want.  I can guess at least one thing
they might want: a fast permit process.  San Francisco has left you
a huge amount of room to beat them in that department.I know restaurants aren't the prime mover though.  The prime mover,
as the Times article said, is cheap housing.  That's a big advantage.
But that phrase "cheap housing" is a bit misleading.  There are
plenty of places that are cheaper.  What's special about Pittsburgh
is not that it's cheap, but that it's a cheap place you'd actually
want to live.Part of that is the buildings themselves. I realized a long time
ago, back when I was a poor twenty-something myself, that the best
deals were places that had once been rich, and then became poor.
If a place has always been rich, it's nice but too expensive.  If
a place has always been poor, it's cheap but grim.  But if a place
was once rich and then got poor, you can find palaces for cheap.
And that's what's bringing people here.  When Pittsburgh was rich,
a hundred years ago, the people who lived here built big solid
buildings.  Not always in the best taste, but definitely solid.  So
here is another piece of advice for becoming a startup hub: don't
destroy the buildings that are bringing people here.  When cities
are on the way back up, like Pittsburgh is now, developers race to
tear down the old buildings.  Don't let that happen.  Focus on
historic preservation.  Big real estate development projects are
not what's bringing the twenty-somethings here.  They're the opposite
of the new restaurants and cafes; they subtract personality from
the city.The empirical evidence suggests you cannot be too strict about
historic preservation.  The tougher cities are about it, the better
they seem to do.But the appeal of Pittsburgh is not just the buildings themselves,
but the neighborhoods they're in. Like San Francisco and New York,
Pittsburgh is fortunate in being a pre-car city.  It's not too
spread out. Because those 25 to 29 year olds do not like driving.
They prefer walking, or bicycling, or taking public transport.  If
you've been to San Francisco recently you can't help noticing the
huge number of bicyclists.  And this is not just a fad that the
twenty-somethings have adopted.  In this respect they have discovered
a better way to live.  The beards will go, but not the bikes.  Cities
where you can get around without driving are just better period.
So I would suggest you do everything you can to capitalize on this.
As with historic preservation, it seems impossible to go too far.Why not make Pittsburgh the most bicycle and pedestrian friendly
city in the country?  See if you can go so far that you make San
Francisco seem backward by comparison.  If you do, it's very unlikely
you'll regret it.  The city will seem like a paradise to the young
people you want to attract.  If they do leave to get jobs elsewhere,
it will be with regret at leaving behind such a place.  And what's
the downside?  Can you imagine a headline "City ruined by becoming
too bicycle-friendly?"  It just doesn't happen.So suppose cool old neighborhoods and cool little restaurants make
this the next Portland.  Will that be enough?  It will put you in
a way better position than Portland itself, because Pittsburgh has
something Portland lacks: a first-rate research university.  CMU
plus little cafes means you have more than hipsters drinking lattes.
It means you have hipsters drinking lattes while talking about
distributed systems.  Now you're getting really close to San
Francisco.In fact you're better off than San Francisco in one way, because
CMU is downtown, but Stanford and Berkeley are out in the suburbs.What can CMU do to help Pittsburgh become a startup hub?  Be an
even better research university.  CMU is one of the best universities
in the world, but imagine what things would be like if it were the
very best, and everyone knew it.  There are a lot of ambitious
people who must go to the best place, wherever it is—if it's in
Siberia.  If CMU were it, they would all come here. There would be
kids in Kazakhstan dreaming of one day living in Pittsburgh.Being that kind of talent magnet is the most important contribution
universities can make toward making their city a startup hub.  In
fact it is practically the only contribution they can make.But wait, shouldn't universities be setting up programs with words
like "innovation" and "entrepreneurship" in their names?  No, they
should not.  These kind of things almost always turn out to be
disappointments.  They're pursuing the wrong targets.  The way to
get innovation is not to aim for innovation but to aim for something
more specific, like better batteries or better 3D printing.  And
the way to learn about entrepreneurship is to do it, which you can't
in school.I know it may disappoint some administrators to hear that the best
thing a university can do to encourage startups is to be a great
university.  It's like telling people who want to lose weight that
the way to do it is to eat less.But if you want to know where startups come from, look at the
empirical evidence.  Look at the histories of the most successful
startups, and you'll find they grow organically out of a couple of
founders building something that starts as an interesting side
project.  Universities are great at bringing together founders, but
beyond that the best thing they can do is get out of the way.  For
example, by not claiming ownership of "intellectual property" that
students and faculty develop, and by having liberal rules about
deferred admission and leaves of absence.In fact, one of the most effective things a university could do to
encourage startups is an elaborate form of getting out of the way
invented by Harvard.  Harvard used to have exams for the fall
semester after Christmas.  At the beginning of January they had
something called "Reading Period" when you were supposed to be
studying for exams.  And Microsoft and Facebook have something in
common that few people realize: they were both started during Reading
Period.  It's the perfect situaton for producing the sort of side
projects that turn into startups. The students are all on campus,
but they don't have to do anything because they're supposed to be
studying for exams.Harvard may have closed this window, because a few years ago they
moved exams before Christmas and shortened reading period from 11
days to 7.  But if a university really wanted to help its students
start startups, the empirical evidence, weighted by market cap,
suggests the best thing they can do is literally nothing.The culture of Pittsburgh is another of its strengths.  It seems
like a city has to be very socially liberal to be a startup hub,
and it's pretty clear why. A city has to tolerate strangeness to
be a home for startups, because startups are so strange.  And you
can't choose to allow just the forms of strangeness that will turn
into big startups, because they're all intermingled.  You have to
tolerate all strangeness.That immediately rules out big chunks of the US.  I'm optimistic
it doesn't rule out Pittsburgh.  One of the things I remember from
growing up here, though I didn't realize at the time that there was
anything unusual about it, is how well people got along.  I'm still
not sure why.  Maybe one reason was that everyone felt like an
immigrant.  When I was a kid in Monroeville, people didn't call
themselves American.  They called themselves Italian or Serbian or
Ukranian.  Just imagine what it must have been like here a hundred
years ago, when people were pouring in from twenty different
countries.  Tolerance was the only option.What I remember about the culture of Pittsburgh is that it was
both tolerant and pragmatic.  That's how I'd describe the culture
of Silicon Valley too.  And it's not a coincidence, because Pittsburgh
was the Silicon Valley of its time.  This was a city where people
built new things.  And while the things people build have changed,
the spirit you need to do that kind of work is the same.So although an influx of latte-swilling hipsters may be annoying
in some ways, I would go out of my way to encourage them.  And more
generally to tolerate strangeness, even unto the degree wacko
Californians do.  For Pittsburgh that is a conservative choice:
it's a return to the city's roots.Unfortunately I saved the toughest part for last. There is one more
thing you need to be a startup hub, and Pittsburgh hasn't got it:
investors.  Silicon Valley has a big investor community because
it's had 50 years to grow one.  New York has a big investor community
because it's full of people who like money a lot and are quick to
notice new ways to get it.  But Pittsburgh has neither of these.
And the cheap housing that draws other people here has no effect
on investors.If an investor community grows up here, it will happen the same way
it did in Silicon Valley: slowly and organically.  So I would not
bet on having a big investor community in the short term.  But
fortunately there are three trends that make that less necessary
than it used to be.  One is that startups are increasingly cheap
to start, so you just don't need as much outside money as you used
to.  The second is that thanks to things like Kickstarter, a startup
can get to revenue faster.  You can put something on Kickstarter
from anywhere.  The third is programs like Y Combinator.  A startup
from anywhere in the world can go to YC for 3 months, pick up
funding, and then return home if they want.My advice is to make Pittsburgh a great place for startups, and
gradually more of them will stick.  Some of those will succeed;
some of their founders will become investors; and still more startups
will stick.This is not a fast path to becoming a startup hub. But it is at
least a path, which is something few other cities have.  And it's
not as if you have to make painful sacrifices in the meantime.
Think about what I've suggested you should do.  Encourage local
restaurants, save old buildings, take advantage of density, make
CMU the best, promote tolerance.  These are the things that make
Pittsburgh good to live in now.  All I'm saying is that you should
do even more of them.And that's an encouraging thought.  If Pittsburgh's path to becoming
a startup hub is to be even more itself, then it has a good chance
of succeeding.  In fact it probably has the best chance of any city
its size.  It will take some effort, and a lot of time, but if any
city can do it, Pittsburgh can.Thanks to Charlie Cheever and Jessica Livingston for reading
drafts of this, and to Meg Cheever for organizing Opt412 and inviting
me to speak.
January 2016Life is short, as everyone knows. When I was a kid I used to wonder
about this. Is life actually short, or are we really complaining
about its finiteness?  Would we be just as likely to feel life was
short if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped
wondering about it.  Then I had kids.  That gave me a way to answer
the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,
into discrete quantities. You only get 52 weekends with your 2 year
old.  If Christmas-as-magic lasts from say ages 3 to 10, you only
get to watch your child experience it 8 times.  And while it's
impossible to say what is a lot or a little of a continuous quantity
like time, 8 is not a lot of something.  If you had a handful of 8
peanuts, or a shelf of 8 books to choose from, the quantity would
definitely seem limited, no matter what your lifespan was.Ok, so life actually is short.  Does it make any difference to know
that?It has for me.  It means arguments of the form "Life is too short
for x" have great force.  It's not just a figure of speech to say
that life is too short for something.  It's not just a synonym for
annoying.  If you find yourself thinking that life is too short for
something, you should try to eliminate it if you can.When I ask myself what I've found life is too short for, the word
that pops into my head is "bullshit." I realize that answer is
somewhat tautological.  It's almost the definition of bullshit that
it's the stuff that life is too short for.  And yet bullshit does
have a distinctive character.  There's something fake about it.
It's the junk food of experience.
[1]If you ask yourself what you spend your time on that's bullshit,
you probably already know the answer.  Unnecessary meetings, pointless
disputes, bureaucracy, posturing, dealing with other people's
mistakes, traffic jams, addictive but unrewarding pastimes.There are two ways this kind of thing gets into your life: it's
either forced on you or it tricks you.  To some extent you have to
put up with the bullshit forced on you by circumstances.  You need
to make money, and making money consists mostly of errands.  Indeed,
the law of supply and demand insures that: the more rewarding some
kind of work is, the cheaper people will do it.  It may be that
less bullshit is forced on you than you think, though.  There has
always been a stream of people who opt out of the default grind and
go live somewhere where opportunities are fewer in the conventional
sense, but life feels more authentic.  This could become more common.You can do it on a smaller scale without moving.  The amount of
time you have to spend on bullshit varies between employers.  Most
large organizations (and many small ones) are steeped in it.  But
if you consciously prioritize bullshit avoidance over other factors
like money and prestige, you can probably find employers that will
waste less of your time.If you're a freelancer or a small company, you can do this at the
level of individual customers.  If you fire or avoid toxic customers,
you can decrease the amount of bullshit in your life by more than
you decrease your income.But while some amount of bullshit is inevitably forced on you, the
bullshit that sneaks into your life by tricking you is no one's
fault but your own.  And yet the bullshit you choose may be harder
to eliminate than the bullshit that's forced on you.  Things that
lure you into wasting your time on them have to be really good at
tricking you.  An example that will be familiar to a lot of people
is arguing online.  When someone
contradicts you, they're in a sense attacking you. Sometimes pretty
overtly.  Your instinct when attacked is to defend yourself.  But
like a lot of instincts, this one wasn't designed for the world we
now live in.  Counterintuitive as it feels, it's better most of
the time not to defend yourself.  Otherwise these people are literally
taking your life.
[2]Arguing online is only incidentally addictive. There are more
dangerous things than that. As I've written before, one byproduct
of technical progress is that things we like tend to become more
addictive.  Which means we will increasingly have to make a conscious
effort to avoid addictions—to stand outside ourselves and ask "is
this how I want to be spending my time?"As well as avoiding bullshit one should actively seek out things
that matter.  But different things matter to different people, and
most have to learn what matters to them.  A few are lucky and realize
early on that they love math or taking care of animals or writing,
and then figure out a way to spend a lot of time doing it.  But
most people start out with a life that's a mix of things that
matter and things that don't, and only gradually learn to distinguish
between them.For the young especially, much of this confusion is induced by the
artificial situations they find themselves in. In middle school and
high school, what the other kids think of you seems the most important
thing in the world.  But when you ask adults what they got wrong
at that age, nearly all say they cared too much what other kids
thought of them.One heuristic for distinguishing stuff that matters is to ask
yourself whether you'll care about it in the future.  Fake stuff
that matters usually has a sharp peak of seeming to matter.  That's
how it tricks you.  The area under the curve is small, but its shape
jabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would
call "important."  Having coffee with a friend matters.  You won't
feel later like that was a waste of time.One great thing about having small children is that they make you
spend time on things that matter: them. They grab your sleeve as
you're staring at your phone and say "will you play with me?" And
odds are that is in fact the bullshit-minimizing option.If life is short, we should expect its shortness to take us by
surprise. And that is just what tends to happen.  You take things
for granted, and then they're gone.  You think you can always write
that book, or climb that mountain, or whatever, and then you realize
the window has closed.  The saddest windows close when other people
die. Their lives are short too.  After my mother died, I wished I'd
spent more time with her.  I lived as if she'd always be there.
And in her typical quiet way she encouraged that illusion.  But an
illusion it was. I think a lot of people make the same mistake I
did.The usual way to avoid being taken by surprise by something is to
be consciously aware of it.  Back when life was more precarious,
people used to be aware of death to a degree that would now seem a
bit morbid.  I'm not sure why, but it doesn't seem the right answer
to be constantly reminding oneself of the grim reaper hovering at
everyone's shoulder.  Perhaps a better solution is to look at the
problem from the other end. Cultivate a habit of impatience about
the things you most want to do. Don't wait before climbing that
mountain or writing that book or visiting your mother.  You don't
need to be constantly reminding yourself why you shouldn't wait.
Just don't wait.I can think of two more things one does when one doesn't have much
of something: try to get more of it, and savor what one has.  Both
make sense here.How you live affects how long you live.  Most people could do better.
Me among them.But you can probably get even more effect by paying closer attention
to the time you have.  It's easy to let the days rush by.  The
"flow" that imaginative people love so much has a darker cousin
that prevents you from pausing to savor life amid the daily slurry
of errands and alarms.  One of the most striking things I've read
was not in a book, but the title of one: James Salter's Burning
the Days.It is possible to slow time somewhat. I've gotten better at it.
Kids help.  When you have small children, there are a lot of moments
so perfect that you can't help noticing.It does help too to feel that you've squeezed everything out of
some experience.  The reason I'm sad about my mother is not just
that I miss her but that I think of all the things we could have
done that we didn't.  My oldest son will be 7 soon.  And while I
miss the 3 year old version of him, I at least don't have any regrets
over what might have been.  We had the best time a daddy and a 3
year old ever had.Relentlessly prune bullshit, don't wait to do things that matter,
and savor the time you have.  That's what you do when life is short.Notes[1]
At first I didn't like it that the word that came to mind was
one that had other meanings.  But then I realized the other meanings
are fairly closely related.  Bullshit in the sense of things you
waste your time on is a lot like intellectual bullshit.[2]
I chose this example deliberately as a note to self.  I get
attacked a lot online.  People tell the craziest lies about me.
And I have so far done a pretty mediocre job of suppressing the
natural human inclination to say "Hey, that's not true!"Thanks to Jessica Livingston and Geoff Ralston for reading drafts
of this.
January 2016Since the 1970s, economic inequality in the US has increased
dramatically. And in particular, the rich have gotten a lot richer.
Nearly everyone who writes about it says that economic inequality
should be decreased.I'm interested in this topic because I was one of the founders of
a company called Y Combinator that helps people start startups.
Almost by definition, if a startup succeeds its founders become
rich. Which means by helping startup founders I've been helping to
increase economic inequality.  If economic inequality should be 
decreased, I shouldn't be helping founders. No one should
be.But that doesn't sound right. So have we just shown, by reductio
ad absurdum, that it's false that economic inequality should be 
decreased?  That doesn't sound right either.   Surely it's bad that some people
are born practically locked into poverty, while at the other extreme
fund managers exploit loopholes to cut their income taxes in half.The solution to this puzzle is to realize that economic inequality
is not just one thing.  It consists of some things that are very
bad, like kids with no chance of reaching their potential, and others
that are good, like Larry Page and Sergey Brin starting the company
you use to find things online.If you want to understand economic inequality—and more importantly,
if you actually want to fix the bad aspects of it—you have to
tease apart the components.  And yet the trend in nearly everything
written about the subject is to do the opposite: to squash together
all the aspects of economic inequality as if it were a single
phenomenon.Sometimes this is done for ideological reasons.  Sometimes it's
because the writer only has very high-level data and so draws
conclusions from that, like the proverbial drunk who looks for his
keys under the lamppost, instead of where he dropped them, because the
light is better there.  Sometimes it's because the writer doesn't
understand critical aspects of inequality, like the role of technology
in wealth creation.  Much of the time, perhaps most of the time,
writing about economic inequality combines all three.
January 2016One advantage of being old is that you can see change happen in
your lifetime.  A lot of the change I've seen is fragmentation.  US
politics is much more polarized than it used to be.  Culturally we
have ever less common ground. The creative class flocks to a handful
of happy cities, abandoning the rest.  And increasing economic
inequality means the spread between rich and poor is growing too.
I'd like to propose a hypothesis: that all these trends are instances
of the same phenomenon.  And moreover, that the cause is not some
force that's pulling us apart, but rather the erosion of forces
that had been pushing us together.Worse still, for those who worry about these trends, the forces
that were pushing us together were an anomaly, a one-time combination
of circumstances that's unlikely to be repeated—and indeed, that
we would not want to repeat.The two forces were war (above all World War II), and the rise of
large corporations.The effects of World War II were both economic and social.
Economically, it decreased variation in income.  Like all modern
armed forces, America's were socialist economically.  From each
according to his ability, to each according to his need.  More or
less.  Higher ranking members of the military got more (as higher
ranking members of socialist societies always do), but what they
got was fixed according to their rank.  And the flattening effect
wasn't limited to those under arms, because the US economy was
conscripted too.  Between 1942 and 1945 all wages were set by the
National War Labor Board. Like the military, they defaulted to
flatness.  And this national standardization of wages was so pervasive
that its effects could still be seen years after the war ended.
[1]Business owners weren't supposed to be making money either.  FDR
said "not a single war millionaire" would be permitted.  To ensure
that, any increase in a company's profits over prewar levels was
taxed at 85%.  And when what was left after corporate taxes reached
individuals, it was taxed again at a marginal rate of 93%.
[2]Socially too the war tended to decrease variation.  Over 16 million
men and women from all sorts of different backgrounds were brought
together in a way of life that was literally uniform.  Service rates
for men born in the early 1920s approached 80%. And working toward
a common goal, often under stress, brought them still closer together.Though strictly speaking World War II lasted less than 4 years for
the US, its effects lasted longer.  Wars make central governments
more powerful, and World War II was an extreme case of this.  In
the US, as in all the other Allied countries, the federal government
was slow to give up the new powers it had acquired.  Indeed, in
some respects the war didn't end in 1945; the enemy just switched
to the Soviet Union.  In tax rates, federal power, defense spending,
conscription, and nationalism the decades after the war looked more
like wartime than prewar peacetime.
[3]
And the social effects
lasted too.  The kid pulled into the army from behind a mule team
in West Virginia didn't simply go back to the farm afterward.
Something else was waiting for him, something that looked a lot
like the army.If total war was the big political story of the 20th century, the
big economic story was the rise of a new kind of company.  And this
too tended to produce both social and economic cohesion.
[4]The 20th century was the century of the big, national corporation.
General Electric, General Foods, General Motors.  Developments in
finance, communications, transportation, and manufacturing enabled
a new type of company whose goal was above all scale.  Version 1
of this world was low-res: a Duplo world of a few giant companies
dominating each big market.
[5]The late 19th and early 20th centuries had been a time of consolidation,
led especially by J. P. Morgan.  Thousands of companies run by their
founders were merged into a couple hundred giant ones run by
professional managers. Economies of scale ruled the day.  It seemed
to people at the time that this was the final state of things.  John
D. Rockefeller said in 1880

  The day of combination is here to stay. Individualism has gone,
  never to return.

He turned out to be mistaken, but he seemed right for the next
hundred years.The consolidation that began in the late 19th century continued for
most of the 20th.  By the end of World War II, as Michael Lind
writes, "the major sectors of the economy were either organized
as government-backed cartels or dominated by a few oligopolistic
corporations."For consumers this new world meant the same choices everywhere, but
only a few of them.  When I grew up there were only 2 or 3 of most
things, and since they were all aiming at the middle of the market
there wasn't much to differentiate them.One of the most important instances of this phenomenon was in TV.
Here there were 3 choices: NBC, CBS, and ABC. Plus public TV for
eggheads and communists.  The programs the 3 networks offered were
indistinguishable.  In fact, here there was a triple pressure toward
the center. If one show did try something daring, local affiliates
in conservative markets would make them stop. Plus since TVs were
expensive whole families watched the same shows together, so they
had to be suitable for everyone.And not only did everyone get the same thing, they got it at the
same time.  It's difficult to imagine now, but every night tens of
millions of families would sit down together in front of their TV
set watching the same show, at the same time, as their next door
neighbors.  What happens now with the Super Bowl used to happen
every night. We were literally in sync.
[6]In a way mid-century TV culture was good. The view it gave of the
world was like you'd find in a children's book, and it probably had
something of the effect that (parents hope) children's books have
in making people behave better.  But, like children's books, TV was
also misleading.  Dangerously misleading, for adults. In his
autobiography, Robert MacNeil talks of seeing gruesome images that
had just come in from Vietnam and thinking, we can't show these to
families while they're having dinner.I know how pervasive the common culture was, because I tried to opt
out of it, and it was practically impossible to find alternatives.
When I was 13 I realized, more from internal evidence than any
outside source, that the ideas we were being fed on TV were crap,
and I stopped watching it.
[7]
But it wasn't just TV.  It seemed
like everything around me was crap.  The politicians all saying the
same things, the consumer brands making almost identical products
with different labels stuck on to indicate how prestigious they
were meant to be, the balloon-frame houses with fake "colonial"
skins, the cars with several feet of gratuitous metal on each end
that started to fall apart after a couple years, the "red delicious"
apples that were red but only nominally 
apples. And in retrospect, it was crap.
[8]But when I went looking for alternatives to fill this void, I found
practically nothing.  There was no Internet then.  The only place
to look was in the chain bookstore in our local shopping mall. 
[9]
There I found a copy of The Atlantic.  I wish I could say it became
a gateway into a wider world, but in fact I found it boring and
incomprehensible.  Like a kid tasting whisky for the first time and
pretending to like it, I preserved that magazine as carefully as
if it had been a book. I'm sure I still have it somewhere.  But
though it was evidence that there was, somewhere, a world that
wasn't red delicious, I didn't find it till college.It wasn't just as consumers that the big companies made us similar.
They did as employers too.  Within companies there were powerful
forces pushing people toward a single model of how to look and act.
IBM was particularly notorious for this, but they were only a little
more extreme than other big companies.  And the models of how to
look and act varied little between companies. Meaning everyone
within this world was expected to seem more or less the same.  And
not just those in the corporate world, but also everyone who aspired
to it—which in the middle of the 20th century meant most people
who weren't already in it.  For most of the 20th century, working-class
people tried hard to look middle class.  You can see it in old
photos.  Few adults aspired to look dangerous in 1950.But the rise of national corporations didn't just compress us
culturally.  It compressed us economically too, and on both ends.Along with giant national corporations, we got giant national labor
unions.  And in the mid 20th century the corporations cut deals
with the unions where they paid over market price for labor.  Partly
because the unions were monopolies. 
[10]
Partly because, as
components of oligopolies themselves, the corporations knew they
could safely pass the cost on to their customers, because their
competitors would have to as well.  And partly because in mid-century
most of the giant companies were still focused on finding new ways
to milk economies of scale.  Just as startups rightly pay AWS a
premium over the cost of running their own servers so they can focus
on growth, many of the big national corporations were willing to
pay a premium for labor. 
[11]As well as pushing incomes up from the bottom, by overpaying unions,
the big companies of the 20th century also pushed incomes down at
the top, by underpaying their top management. Economist J. K.
Galbraith wrote in 1967 that "There are few corporations in which
it would be suggested that executive salaries are at a maximum."
[12]To some extent this was an illusion.  Much of the de facto pay of
executives never showed up on their income tax returns, because it
took the form of perks.  The higher the rate of income tax, the
more pressure there was to pay employees upstream of it.  (In the
UK, where taxes were even higher than in the US, companies would
even pay their kids' private school tuitions.)  One of the most
valuable things the big companies of the mid 20th century gave their
employees was job security, and this too didn't show up in tax
returns or income statistics. So the nature of employment in these
organizations tended to yield falsely low numbers about economic
inequality.  But even accounting for that, the big companies paid
their best people less than market price.  There was no market; the
expectation was that you'd work for the same company for decades
if not your whole career. 
[13]Your work was so illiquid there was little chance of getting market
price. But that same illiquidity also encouraged you not to seek
it.  If the company promised to employ you till you retired and
give you a pension afterward, you didn't want to extract as much
from it this year as you could. You needed to take care of the
company so it could take care of you.  Especially when you'd been
working with the same group of people for decades.  If you tried
to squeeze the company for more money, you were squeezing the
organization that was going to take care of them.  Plus if
you didn't put the company first you wouldn't be promoted, and if
you couldn't switch ladders, promotion on this one was the only way
up. 
[14]To someone who'd spent several formative years in the armed forces,
this situation didn't seem as strange as it does to us now.  From
their point of view, as big company executives, they were high-ranking
officers.  They got paid a lot more than privates.  They got to
have expense account lunches at the best restaurants and fly around
on the company's Gulfstreams.  It probably didn't occur to most of
them to ask if they were being paid market price.The ultimate way to get market price is to work for yourself, by
starting your own company.  That seems obvious to any ambitious
person now.  But in the mid 20th century it was an alien concept.
Not because starting one's own company seemed too ambitious, but
because it didn't seem ambitious enough. Even as late as the 1970s,
when I grew up, the ambitious plan was to get lots of education at
prestigious institutions, and then join some other prestigious
institution and work one's way up the hierarchy.  Your prestige was
the prestige of the institution you belonged to.  People did start
their own businesses of course, but educated people rarely did,
because in those days there was practically zero concept of starting
what we now call a startup: 
a business that starts small and grows
big.  That was much harder to do in the mid 20th century.  Starting
one's own business meant starting a business that would start small
and stay small. Which in those days of big companies often meant
scurrying around trying to avoid being trampled by elephants.  It
was more prestigious to be one of the executive class riding the
elephant.By the 1970s, no one stopped to wonder where the big prestigious
companies had come from in the first place.  It seemed like they'd
always been there, like the chemical elements.  And indeed, there
was a double wall between ambitious kids in the 20th century and
the origins of the big companies.  Many of the big companies were
roll-ups that didn't have clear founders.  And when they did, the
founders didn't seem like us.  Nearly all of them had been uneducated,
in the sense of not having been to college.  They were what Shakespeare
called rude mechanicals.  College trained one to be a member of the
professional classes.  Its graduates didn't expect to do the sort
of grubby menial work that Andrew Carnegie or Henry Ford started
out doing. 
[15]And in the 20th century there were more and more college graduates.
They increased from about 2% of the population in 1900 to about 25%
in 2000. In the middle of the century our two big forces intersect,
in the form of the GI Bill, which sent 2.2 million World War II
veterans to college.  Few thought of it in these terms, but the
result of making college the canonical path for the ambitious was
a world in which it was socially acceptable to work for Henry Ford,
but not to be Henry Ford.
[16]I remember this world well. I came of age just as it was starting
to break up. In my childhood it was still dominant. Not quite so
dominant as it had been.  We could see from old TV shows and yearbooks
and the way adults acted that people in the 1950s and 60s had been
even more conformist than us.  The mid-century model was already
starting to get old. But that was not how we saw it at the time.
We would at most have said that one could be a bit more daring in
1975 than 1965.  And indeed, things hadn't changed much yet.But change was coming soon. And when the Duplo economy started to
disintegrate, it disintegrated in several different ways at once.
Vertically integrated companies literally dis-integrated because
it was more efficient to.  Incumbents faced new competitors as (a)
markets went global and (b) technical innovation started to trump
economies of scale, turning size from an asset into a liability.
Smaller companies were increasingly able to survive as formerly
narrow channels to consumers broadened.  Markets themselves started
to change faster, as whole new categories of products appeared. And
last but not least, the federal government, which had previously
smiled upon J. P. Morgan's world as the natural state of things,
began to realize it wasn't the last word after all.What J. P. Morgan was to the horizontal axis, Henry Ford was to the
vertical.  He wanted to do everything himself. The giant plant he
built at River Rouge between 1917 and 1928 literally took in iron
ore at one end and sent cars out the other.  100,000 people worked
there. At the time it seemed the future. But that is not how car
companies operate today.  Now much of the design and manufacturing
happens in a long supply chain, whose products the car companies
ultimately assemble and sell.  The reason car companies operate
this way is that it works better.  Each company in the supply chain
focuses on what they know best. And they each have to do it well
or they can be swapped out for another supplier.Why didn't Henry Ford realize that networks of cooperating companies
work better than a single big company? One reason is that supplier
networks take a while to evolve. In 1917, doing everything himself
seemed to Ford the only way to get the scale he needed. And the
second reason is that if you want to solve a problem using a network
of cooperating companies, you have to be able to coordinate their
efforts, and you can do that much better with computers.  Computers
reduce the transaction costs that Coase argued are the raison d'etre
of corporations. That is a fundamental change.In the early 20th century, big companies were synonymous with
efficiency.  In the late 20th century they were synonymous with
inefficiency.  To some extent this was because the companies
themselves had become sclerotic.  But it was also because our
standards were higher.It wasn't just within existing industries that change occurred.
The industries themselves changed.  It became possible to make lots
of new things, and sometimes the existing companies weren't the
ones who did it best.Microcomputers are a classic example. The market was pioneered by
upstarts like Apple. When it got big enough, IBM decided it was
worth paying attention to.  At the time IBM completely dominated
the computer industry. They assumed that all they had to do, now
that this market was ripe, was to reach out and pick it.  Most
people at the time would have agreed with them.  But what happened
next illustrated how much more complicated the world had become.
IBM did launch a microcomputer.  Though quite successful, it did
not crush Apple.  But even more importantly, IBM itself ended up
being supplanted by a supplier coming in from the side—from
software, which didn't even seem to be the same business.  IBM's
big mistake was to accept a non-exclusive license for DOS.  It must
have seemed a safe move at the time.  No other computer manufacturer
had ever been able to outsell them. What difference did it make if
other manufacturers could offer DOS too?  The result of that
miscalculation was an explosion of inexpensive PC clones.  Microsoft
now owned the PC standard, and the customer.  And the microcomputer
business ended up being Apple vs Microsoft.Basically, Apple bumped IBM and then Microsoft stole its wallet.
That sort of thing did not happen to big companies in mid-century.
But it was going to happen increasingly often in the future.Change happened mostly by itself in the computer business.  In other
industries, legal obstacles had to be removed first.  Many of the
mid-century oligopolies had been anointed by the federal government
with policies (and in wartime, large orders) that kept out competitors.
This didn't seem as dubious to government officials at the time as
it sounds to us. They felt a two-party system ensured sufficient
competition in politics.  It ought to work for business too.Gradually the government realized that anti-competitive policies
were doing more harm than good, and during the Carter administration
it started to remove them. The word used for this process was
misleadingly narrow: deregulation.  What was really happening was
de-oligopolization.  It happened to one industry after another.
Two of the most visible to consumers were air travel and long-distance
phone service, which both became dramatically cheaper after
deregulation.Deregulation also contributed to the wave of hostile takeovers in
the 1980s.  In the old days the only limit on the inefficiency of
companies, short of actual bankruptcy, was the inefficiency of their
competitors.  Now companies had to face absolute rather than relative
standards.  Any public company that didn't generate sufficient
returns on its assets risked having its management replaced with
one that would.  Often the new managers did this by breaking companies
up into components that were more valuable separately.
[17]Version 1 of the national economy consisted of a few big blocks
whose relationships were negotiated in back rooms by a handful of
executives, politicians, regulators, and labor leaders.  Version 2
was higher resolution: there were more companies, of more different
sizes, making more different things, and their relationships changed
faster. In this world there were still plenty of back room negotiations,
but more was left to market forces.  Which further accelerated the
fragmentation.It's a little misleading to talk of versions when describing a
gradual process, but not as misleading as it might seem.  There was
a lot of change in a few decades, and what we ended up with was
qualitatively different.  The companies in the S&P 500 in 1958 had
been there an average of 61 years. By 2012 that number was 18 years.
[18]The breakup of the Duplo economy happened simultaneously with the
spread of computing power. To what extent were computers a precondition?
It would take a book to answer that. Obviously the spread of computing
power was a precondition for the rise of startups.  I suspect it
was for most of what happened in finance too.  But was it a
precondition for globalization or the LBO wave?  I don't know, but
I wouldn't discount the possibility.  It may be that the refragmentation
was driven by computers in the way the industrial revolution was
driven by steam engines.  Whether or not computers were a precondition,
they have certainly accelerated it.The new fluidity of companies changed people's relationships with
their employers. Why climb a corporate ladder that might be yanked
out from under you?  Ambitious people started to think of a career
less as climbing a single ladder than as a series of jobs that might
be at different companies. More movement (or even potential movement)
between companies introduced more competition in salaries.  Plus
as companies became smaller it became easier to estimate how much
an employee contributed to the company's revenue.  Both changes
drove salaries toward market price. And since people vary dramatically
in productivity, paying market price meant salaries started to
diverge.By no coincidence it was in the early 1980s that the term "yuppie"
was coined.  That word is not much used now, because the phenomenon
it describes is so taken for granted, but at the time it was a label
for something novel. Yuppies were young professionals who made lots
of money.  To someone in their twenties today, this wouldn't seem
worth naming.  Why wouldn't young professionals make lots of money?
But until the 1980s being underpaid early in your career was part
of what it meant to be a professional.  Young professionals were
paying their dues, working their way up the ladder.  The rewards
would come later.  What was novel about yuppies was that they wanted
market price for the work they were doing now.The first yuppies did not work for startups. That was still in the
future.  Nor did they work for big companies. They were professionals
working in fields like law, finance, and consulting.  But their example 
rapidly inspired their peers.  Once they saw that new BMW 325i, they 
wanted one too.Underpaying people at the beginning of their career only works if
everyone does it. Once some employer breaks ranks, everyone else
has to, or they can't get good people.  And once started this process
spreads through the whole economy, because at the beginnings of
people's careers they can easily switch not merely employers but
industries.But not all young professionals benefitted. You had to produce to
get paid a lot.  It was no coincidence that the first yuppies worked
in fields where it was easy to measure that.More generally, an idea was returning whose name sounds old-fashioned
precisely because it was so rare for so long: that you could make
your fortune.  As in the past there were multiple ways to do it.
Some made their fortunes by creating wealth, and others by playing
zero-sum games. But once it became possible to make one's fortune,
the ambitious had to decide whether or not to.  A physicist who
chose physics over Wall Street in 1990 was making a sacrifice that
a physicist in 1960 wasn't.The idea even flowed back into big companies.  CEOs of big companies
make more now than they used to, and I think much of the reason is
prestige.  In 1960, corporate CEOs had immense prestige.  They were
the winners of the only economic game in town. But if they made as
little now as they did then, in real dollar terms, they'd seem like
small fry compared to professional athletes and whiz kids making
millions from startups and hedge funds. They don't like that idea,
so now they try to get as much as they can, which is more than they
had been getting. 
[19]Meanwhile a similar fragmentation was happening at the other end
of the economic scale.  As big companies' oligopolies became less
secure, they were less able to pass costs on to customers and thus
less willing to overpay for labor.  And as the Duplo world of a few
big blocks fragmented into many companies of different sizes—some
of them overseas—it became harder for unions to enforce their
monopolies.  As a result workers' wages also tended toward market
price. Which (inevitably, if unions had been doing their job) tended
to be lower.  Perhaps dramatically so, if automation had decreased
the need for some kind of work.And just as the mid-century model induced social as well as economic
cohesion, its breakup brought social as well as economic fragmentation.
People started to dress and act differently.  Those who would later
be called the "creative class" became more mobile. People who didn't
care much for religion felt less pressure to go to church for
appearances' sake, while those who liked it a lot opted for
increasingly colorful forms. Some switched from meat loaf to tofu,
and others to Hot Pockets. Some switched from driving Ford sedans
to driving small imported cars, and others to driving SUVs.  Kids
who went to private schools or wished they did started to dress
"preppy," and kids who wanted to seem rebellious made a conscious
effort to look disreputable.  In a hundred ways people spread apart.
[20]Almost four decades later, fragmentation is still increasing.  Has
it been net good or bad?  I don't know; the question may be
unanswerable.  Not entirely bad though.  We take for granted the
forms of fragmentation we like, and worry only about the ones we
don't. But as someone who caught the tail end of mid-century
conformism, 
I can tell you it was no utopia.
[21]My goal here is not to say whether fragmentation has been good or
bad, just to explain why it's happening.  With the centripetal
forces of total war and 20th century oligopoly mostly gone, what
will happen next?  And more specifically, is it possible to reverse
some of the fragmentation we've seen?If it is, it will have to happen piecemeal.  You can't reproduce
mid-century cohesion the way it was originally produced.  It would
be insane to go to war just to induce more national unity.  And
once you understand the degree to which the economic history of the
20th century was a low-res version 1, it's clear you can't reproduce
that either.20th century cohesion was something that happened at least in a
sense naturally.  The war was due mostly to external forces, and
the Duplo economy was an evolutionary phase.  If you want cohesion
now, you'd have to induce it deliberately.  And it's not obvious
how.  I suspect the best we'll be able to do is address the symptoms
of fragmentation.  But that may be enough.The form of fragmentation people worry most about lately is economic inequality, and if you want to eliminate
that you're up against a truly formidable headwind—one that has
been in operation since the stone age: technology.  Technology is
a lever. It magnifies work.  And the lever not only grows increasingly
long, but the rate at which it grows is itself increasing.Which in turn means the variation in the amount of wealth people
can create has not only been increasing, but accelerating.  The
unusual conditions that prevailed in the mid 20th century masked
this underlying trend.  The ambitious had little choice but to join
large organizations that made them march in step with lots of other
people—literally in the case of the armed forces, figuratively
in the case of big corporations. Even if the big corporations had
wanted to pay people proportionate to their value, they couldn't
have figured out how.  But that constraint has gone now.  Ever since
it started to erode in the 1970s, we've seen the underlying forces
at work again.
[22]Not everyone who gets rich now does it by creating wealth, certainly.
But a significant number do, and the Baumol Effect means all their
peers get dragged along too.
[23]
And as long as it's possible to
get rich by creating wealth, the default tendency will be for
economic inequality to increase.  Even if you eliminate all the
other ways to get rich.  You can mitigate this with subsidies at
the bottom and taxes at the top, but unless taxes are high enough
to discourage people from creating wealth, you're always going to
be fighting a losing battle against increasing variation in
productivity.
[24]That form of fragmentation, like the others, is here to stay.  Or
rather, back to stay.  Nothing is forever, but the tendency toward
fragmentation should be more forever than most things, precisely
because it's not due to any particular cause.  It's simply a reversion
to the mean. When Rockefeller said individualism was gone, he was
right for a hundred years.  It's back now, and that's likely to be
true for longer.I worry that if we don't acknowledge this, we're headed for trouble.
If we think 20th century cohesion disappeared because of few policy
tweaks, we'll be deluded into thinking we can get it back (minus
the bad parts, somehow) with a few countertweaks.  And then we'll
waste our time trying to eliminate fragmentation, when we'd be
better off thinking about how to mitigate its consequences.
Notes[1]
Lester Thurow, writing in 1975, said the wage differentials
prevailing at the end of World War II had become so embedded that
they "were regarded as 'just' even after the egalitarian pressures
of World War II had disappeared.  Basically, the same differentials
exist to this day, thirty years later." But Goldin and Margo think
market forces in the postwar period also helped preserve the wartime
compression of wages—specifically increased demand for unskilled
workers, and oversupply of educated ones.(Oddly enough, the American custom of having employers pay for
health insurance derives from efforts by businesses to circumvent
NWLB wage controls in order to attract workers.)[2]
As always, tax rates don't tell the whole story.  There were
lots of exemptions, especially for individuals.  And in World War
II the tax codes were so new that the government had little acquired
immunity to tax avoidance.  If the rich paid high taxes during the
war it was more because they wanted to than because they had to.After the war, federal tax receipts as a percentage of GDP were
about the same as they are now. In fact, for the entire period since
the war, tax receipts have stayed close to 18% of GDP, despite
dramatic changes in tax rates.  The lowest point occurred when
marginal income tax rates were highest: 14.1% in 1950.  Looking at
the data, it's hard to avoid the conclusion that tax rates have had
little effect on what people actually paid.[3]
Though in fact the decade preceding the war had been a time
of unprecedented federal power, in response to the Depression.
Which is not entirely a coincidence, because the Depression was one
of the causes of the war.  In many ways the New Deal was a sort of
dress rehearsal for the measures the federal government took during
wartime.  The wartime versions were much more drastic and more
pervasive though.  As Anthony Badger wrote, "for many Americans the
decisive change in their experiences came not with the New Deal but
with World War II."[4]
I don't know enough about the origins of the world wars to
say, but it's not inconceivable they were connected to the rise of
big corporations. If that were the case, 20th century cohesion would
have a single cause.[5]
More precisely, there was a bimodal economy consisting, in
Galbraith's words, of "the world of the technically dynamic, massively
capitalized and highly organized corporations on the one hand and
the hundreds of thousands of small and traditional proprietors on
the other." Money, prestige, and power were concentrated in the
former, and there was near zero crossover.[6]
I wonder how much of the decline in families eating together
was due to the decline in families watching TV together afterward.[7]
I know when this happened because it was the season Dallas
premiered.  Everyone else was talking about what was happening on
Dallas, and I had no idea what they meant.[8]
I didn't realize it till I started doing research for this
essay, but the meretriciousness of the products I grew up with is
a well-known byproduct of oligopoly. When companies can't compete
on price, they compete on tailfins.[9]
Monroeville Mall was at the time of its completion in 1969
the largest in the country. In the late 1970s the movie Dawn of
the Dead was shot there. Apparently the mall was not just the
location of the movie, but its inspiration; the crowds of shoppers
drifting through this huge mall reminded George Romero of zombies.
My first job was scooping ice cream in the Baskin-Robbins.[10]
Labor unions were exempted from antitrust laws by the Clayton
Antitrust Act in 1914 on the grounds that a person's work is not
"a commodity or article of commerce." I wonder if that means service
companies are also exempt.[11]
The relationships between unions and unionized companies can
even be symbiotic, because unions will exert political pressure to
protect their hosts.  According to Michael Lind, when politicians
tried to attack the A&P supermarket chain because it was putting
local grocery stores out of business, "A&P successfully defended
itself by allowing the unionization of its workforce in 1938, thereby
gaining organized labor as a constituency." I've seen this phenomenon
myself: hotel unions are responsible for more of the political
pressure against Airbnb than hotel companies.[12]
Galbraith was clearly puzzled that corporate executives would
work so hard to make money for other people (the shareholders)
instead of themselves.  He devoted much of The New Industrial
State to trying to figure this out.His theory was that professionalism had replaced money as a motive,
and that modern corporate executives were, like (good) scientists,
motivated less by financial rewards than by the desire to do good
work and thereby earn the respect of their peers.  There is something
in this, though I think lack of movement between companies combined
with self-interest explains much of observed behavior.[13]
Galbraith (p. 94) says a 1952 study of the 800 highest paid
executives at 300 big corporations found that three quarters of
them had been with their company for more than 20 years.[14]
It seems likely that in the first third of the 20th century
executive salaries were low partly because companies then were more
dependent on banks, who would have disapproved if executives got
too much.  This was certainly true in the beginning. The first big
company CEOs were J. P. Morgan's hired hands.Companies didn't start to finance themselves with retained earnings
till the 1920s.  Till then they had to pay out their earnings in
dividends, and so depended on banks for capital for expansion.
Bankers continued to sit on corporate boards till the Glass-Steagall
act in 1933.By mid-century big companies funded 3/4 of their growth from earnings.
But the early years of bank dependence, reinforced by the financial
controls of World War II, must have had a big effect on social
conventions about executive salaries.  So it may be that the lack
of movement between companies was as much the effect of low salaries
as the cause.Incidentally, the switch in the 1920s to financing growth with
retained earnings was one cause of the 1929 crash.  The banks now
had to find someone else to lend to, so they made more margin loans.[15]
Even now it's hard to get them to. One of the things I find
hardest to get into the heads of would-be startup founders is how
important it is to do certain kinds of menial work early in the
life of a company.  Doing things that don't
scale is to how Henry Ford got started as a high-fiber diet is
to the traditional peasant's diet: they had no choice but to do the
right thing, while we have to make a conscious effort.[16]
Founders weren't celebrated in the press when I was a kid.
"Our founder" meant a photograph of a severe-looking man with a
walrus mustache and a wing collar who had died decades ago. The
thing to be when I was a kid was an executive. If you weren't
around then it's hard to grasp the cachet that term had. The fancy
version of everything was called the "executive" model.[17]
The wave of hostile takeovers in the 1980s was enabled by a
combination of circumstances: court decisions striking down state
anti-takeover laws, starting with the Supreme Court's 1982 decision
in Edgar v. MITE Corp.; the Reagan administration's comparatively
sympathetic attitude toward takeovers; the Depository Institutions
Act of 1982, which allowed banks and savings and loans to buy
corporate bonds; a new SEC rule issued in 1982 (rule 415) that made
it possible to bring corporate bonds to market faster; the creation
of the junk bond business by Michael Milken; a vogue for conglomerates
in the preceding period that caused many companies to be combined
that never should have been; a decade of inflation that left many
public companies trading below the value of their assets; and not
least, the increasing complacency of managements.[18]
Foster, Richard. "Creative Destruction Whips through Corporate
America." Innosight, February 2012.[19]
CEOs of big companies may be overpaid. I don't know enough
about big companies to say. But it is certainly not impossible for
a CEO to make 200x as much difference to a company's revenues as
the average employee.  Look at what Steve Jobs did for Apple when
he came back as CEO.  It would have been a good deal for the board
to give him 95% of the company.  Apple's market cap the day Steve
came back in July 1997 was 1.73 billion. 5% of Apple now (January
2016) would be worth about 30 billion.  And it would not be if Steve
hadn't come back; Apple probably wouldn't even exist anymore.Merely including Steve in the sample might be enough to answer the
question of whether public company CEOs in the aggregate are overpaid.
And that is not as facile a trick as it might seem, because the
broader your holdings, the more the aggregate is what you care
about.[20]
The late 1960s were famous for social upheaval. But that was
more rebellion (which can happen in any era if people are provoked
sufficiently) than fragmentation.  You're not seeing fragmentation
unless you see people breaking off to both left and right.[21]
Globally the trend has been in the other direction.  While
the US is becoming more fragmented, the world as a whole is becoming
less fragmented, and mostly in good ways.[22]
There were a handful of ways to make a fortune in the mid
20th century.  The main one was drilling for oil, which was open
to newcomers because it was not something big companies could
dominate through economies of scale.  How did individuals accumulate
large fortunes in an era of such high taxes?  Giant tax loopholes
defended by two of the most powerful men in Congress, Sam Rayburn
and Lyndon Johnson.But becoming a Texas oilman was not in 1950 something one could
aspire to the way starting a startup or going to work on Wall Street
were in 2000, because (a) there was a strong local component and
(b) success depended so much on luck.[23]
The Baumol Effect induced by startups is very visible in
Silicon Valley.  Google will pay people millions of dollars a year
to keep them from leaving to start or join startups.[24]
I'm not claiming variation in productivity is the only cause
of economic inequality in the US. But it's a significant cause, and
it will become as big a cause as it needs to, in the sense that if
you ban other ways to get rich, people who want to get rich will
use this route instead.Thanks to Sam Altman, Trevor Blackwell, Paul Buchheit, Patrick
Collison, Ron Conway, Chris Dixon, Benedict Evans, Richard Florida,
Ben Horowitz, Jessica Livingston, Robert Morris, Tim O'Reilly, Geoff
Ralston, Max Roser, Alexia Tsotsis, and Qasar Younis for reading
drafts of this.  Max also told me about several valuable sources.BibliographyAllen, Frederick Lewis. The Big Change. Harper, 1952.Averitt, Robert. The Dual Economy. Norton, 1968.Badger, Anthony. The New Deal. Hill and Wang, 1989.Bainbridge, John. The Super-Americans. Doubleday, 1961.Beatty, Jack. Collossus. Broadway, 2001.Brinkley, Douglas. Wheels for the World. Viking, 2003.Brownleee, W. Elliot. Federal Taxation in America. Cambridge, 1996.Chandler, Alfred. The Visible Hand. Harvard, 1977.Chernow, Ron. The House of Morgan. Simon & Schuster, 1990.Chernow, Ron. Titan: The Life of John D. Rockefeller. Random House,
1998.Galbraith, John. The New Industrial State. Houghton Mifflin, 1967.Goldin, Claudia and Robert A. Margo. "The Great Compression: The
Wage Structure in the United States at Mid-Century." NBER Working
Paper 3817, 1991.Gordon, John. An Empire of Wealth. HarperCollins, 2004.Klein, Maury. The Genesis of Industrial America, 1870-1920. Cambridge,
2007.Lind, Michael. Land of Promise. HarperCollins, 2012.Mickelthwaite, John, and Adrian Wooldridge. The Company. Modern
Library, 2003.Nasaw, David. Andrew Carnegie. Penguin, 2006.Sobel, Robert. The Age of Giant Corporations. Praeger, 1993.Thurow, Lester. Generating Inequality: Mechanisms of Distribution.
Basic Books, 1975.Witte, John. The Politics and Development of the Federal Income
Tax. Wisconsin, 1985.Related:
November 2015A few months ago an article about Y Combinator said that early on
it had been a "one-man show."  It's sadly common to read that sort
of thing.  But the problem with that description is not just that
it's unfair.  It's also misleading.  Much of what's most novel about
YC is due to Jessica Livingston.  If you don't understand her, you
don't understand YC.  So let me tell you a little about Jessica.YC had 4 founders.  Jessica and I decided one night to start it,
and the next day we recruited my friends Robert Morris and Trevor
Blackwell.  Jessica and I ran YC day to day, and Robert and Trevor
read applications and did interviews with us.Jessica and I were already dating when we started YC.  At first we
tried to act "professional" about this, meaning we tried to conceal
it.  In retrospect that seems ridiculous, and we soon dropped the
pretense.  And the fact that Jessica and I were a couple is a big
part of what made YC what it was.  YC felt like a family.  The
founders early on were mostly young.  We all had dinner together
once a week, cooked for the first couple years by me. Our first
building had been a private home.  The overall atmosphere was
shockingly different from a VC's office on Sand Hill Road, in a way
that was entirely for the better.  There was an authenticity that
everyone who walked in could sense.  And that didn't just mean that
people trusted us.  It was the perfect quality to instill in startups.
Authenticity is one of the most important things YC looks for in
founders, not just because fakers and opportunists are annoying,
but because authenticity is one of the main things that separates
the most successful startups from the rest.Early YC was a family, and Jessica was its mom.  And the culture
she defined was one of YC's most important innovations.  Culture
is important in any organization, but at YC culture wasn't just how
we behaved when we built the product. At YC, the culture was the
product.Jessica was also the mom in another sense: she had the last word.
Everything we did as an organization went through her first—who
to fund, what to say to the public, how to deal with other companies,
who to hire, everything.Before we had kids, YC was more or less our life. There was no real
distinction between working hours and not.  We talked about YC all
the time.  And while there might be some businesses that it would
be tedious to let infect your private life, we liked it. We'd started
YC because it was something we were interested in.  And some of the
problems we were trying to solve were endlessly difficult.  How do
you recognize good founders?  You could talk about that for years,
and we did; we still do.I'm better at some things than Jessica, and she's better at some
things than me.  One of the things she's best at is judging people.
She's one of those rare individuals with x-ray vision for character.
She can see through any kind of faker almost immediately.  Her
nickname within YC was the Social Radar, and this special power of
hers was critical in making YC what it is.  The earlier you pick
startups, the more you're picking the founders.  Later stage investors
get to try products and look at growth numbers.  At the stage where
YC invests, there is often neither a product nor any numbers.Others thought YC had some special insight about the future of
technology.  Mostly we had the same sort of insight Socrates claimed:
we at least knew we knew nothing.  What made YC successful was being
able to pick good founders.  We thought Airbnb was a bad idea.  We
funded it because we liked the founders.During interviews, Robert and Trevor and I would pepper the applicants
with technical questions.  Jessica would mostly watch.  A lot of
the applicants probably read her as some kind of secretary, especially
early on, because she was the one who'd go out and get each new
group and she didn't ask many questions. She was ok with that.  It
was easier for her to watch people if they didn't notice her. But
after the interview, the three of us would turn to Jessica and ask
"What does the Social Radar say?"
[1]Having the Social Radar at interviews wasn't just how we picked
founders who'd be successful.  It was also how we picked founders
who were good people.  At first we did this because we couldn't
help it.  Imagine what it would feel like to have x-ray vision for
character.  Being around bad people would be intolerable.  So we'd
refuse to fund founders whose characters we had doubts about even
if we thought they'd be successful.Though we initially did this out of self-indulgence, it turned out
to be very valuable to YC.  We didn't realize it in the beginning,
but the people we were picking would become the YC alumni network.
And once we picked them, unless they did something really egregious,
they were going to be part of it for life. Some now think YC's
alumni network is its most valuable feature. I personally think
YC's advice is pretty good too, but the alumni network is certainly
among the most valuable features.  The level of trust and helpfulness
is remarkable for a group of such size.  And Jessica is the main
reason why.(As we later learned, it probably cost us little to reject people
whose characters we had doubts about, because how good founders are
and how well they do are not orthogonal.  If bad founders succeed
at all, they tend to sell early.  The most successful founders are
almost all good.)If Jessica was so important to YC, why don't more people realize
it?  Partly because I'm a writer, and writers always get disproportionate
attention.  YC's brand was initially my brand, and our applicants
were people who'd read my essays.  But there is another reason:
Jessica hates attention.  Talking to reporters makes her nervous.
The thought of giving a talk paralyzes her.  She was even uncomfortable
at our wedding, because the bride is always the center of attention.
[2]It's not just because she's shy that she hates attention, but because
it throws off the Social Radar. She can't be herself. You can't
watch people when everyone is watching you.Another reason attention worries her is that she hates bragging.
In anything she does that's publicly visible, her biggest fear
(after the obvious fear that it will be bad) is that it will seem
ostentatious.  She says being too modest is a common problem for
women.  But in her case it goes beyond that.  She has a horror of
ostentation so visceral it's almost a phobia.She also hates fighting. She can't do it; she just shuts down.  And
unfortunately there is a good deal of fighting in being the public
face of an organization.So although Jessica more than anyone made YC unique, the very
qualities that enabled her to do it mean she tends to get written
out of YC's history.  Everyone buys this story that PG started YC
and his wife just kind of helped.  Even YC's haters buy it.  A
couple years ago when people were attacking us for not funding more
female founders (than exist), they all treated YC as identical with
PG.  It would have spoiled the narrative to acknowledge Jessica's
central role at YC.Jessica was boiling mad that people were accusing her company of
sexism. I've never seen her angrier about anything.  But she did
not contradict them.  Not publicly.  In private there was a great
deal of profanity.  And she wrote three separate essays about the
question of female founders.  But she could never bring herself to
publish any of them.  She'd seen the level of vitriol in this debate,
and she shrank from engaging.
[3]It wasn't just because she disliked fighting.  She's so sensitive
to character that it repels her even to fight with dishonest people.
The idea of mixing it up with linkbait journalists or Twitter trolls
would seem to her not merely frightening, but disgusting.But Jessica knew her example as a successful female founder would
encourage more women to start companies, so last year she did
something YC had never done before and hired a PR firm to get her
some interviews.  At one of the first she did, the reporter brushed
aside her insights about startups and turned it into a sensationalistic
story about how some guy had tried to chat her up as she was waiting
outside the bar where they had arranged to meet.  Jessica was
mortified, partly because the guy had done nothing wrong, but more
because the story treated her as a victim significant only for being
a woman, rather than one of the most knowledgeable investors in the
Valley.After that she told the PR firm to stop.You're not going to be hearing in the press about what Jessica has
achieved. So let me tell you what Jessica has achieved.  Y Combinator
is fundamentally a nexus of people, like a university. It doesn't
make a product. What defines it is the people.  Jessica more than
anyone curated and nurtured that collection of people.  In that
sense she literally made YC.Jessica knows more about the qualities of startup founders than
anyone else ever has. Her immense data set and x-ray vision are the
perfect storm in that respect.  The qualities of the founders are
the best predictor of how a startup will do.  And startups are in
turn the most important source of growth in mature economies.The person who knows the most about the most important factor in
the growth of mature economies—that is who Jessica Livingston is.
Doesn't that sound like someone who should be better known?Notes[1]
Harj Taggar reminded me that while Jessica didn't ask many
questions, they tended to be important ones:"She was always good at sniffing out any red flags about the team
or their determination and disarmingly asking the right question,
which usually revealed more than the founders realized."[2]
Or more precisely, while she likes getting attention in the
sense of getting credit for what she has done, she doesn't like
getting attention in the sense of being watched in real time.
Unfortunately, not just for her but for a lot of people, how much
you get of the former depends a lot on how much you get of the
latter.Incidentally, if you saw Jessica at a public event, you would never
guess she
hates attention, because (a) she is very polite and (b) when she's
nervous, she expresses it by smiling more.[3]
The existence of people like Jessica is not just something
the mainstream media needs to learn to acknowledge, but something
feminists need to learn to acknowledge as well.  There are successful
women who don't like to fight.  Which means if the public conversation
about women consists of fighting, their voices will be silenced.There's a sort of Gresham's Law of conversations. If a conversation
reaches a certain level of incivility, the more thoughtful people
start to leave. No one understands female founders better than
Jessica.  But it's unlikely anyone will ever hear her speak candidly
about the topic. She ventured a toe in that water a while ago, and
the reaction was so violent that she decided "never again."
Thanks to Sam Altman, Paul Buchheit, Patrick Collison, 
Daniel Gackle, Carolynn
Levy, Jon Levy, Kirsty Nathoo, Robert Morris, Geoff Ralston, and
Harj Taggar for reading drafts of this.  And yes, Jessica Livingston,
who made me cut surprisingly little.
October 2015This will come as a surprise to a lot of people, but in some cases
it's possible to detect bias in a selection process without knowing
anything about the applicant pool.  Which is exciting because among
other things it means third parties can use this technique to detect
bias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least
a random sample of the applicants that were selected, (b) their
subsequent performance is measured, and (c) the groups of
applicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What
it means for a selection process to be biased against applicants
of type x is that it's harder for them to make it through.  Which
means applicants of type x have to be better to get selected than
applicants not of type x.
[1]
Which means applicants of type x
who do make it through the selection process will outperform other
successful applicants.  And if the performance of all the successful
applicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid
one.  And in particular it must not be invalidated by the bias you're
trying to measure.
But there are some domains where performance can be measured, and
in those detecting bias is straightforward. Want to know if the
selection process was biased against some type of applicant?  Check
whether they outperform the others.  This is not just a heuristic
for detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased
against female founders. This would be easy to detect: among their
portfolio companies, do startups with female founders outperform
those without?  A couple months ago, one VC firm (almost certainly
unintentionally) published a study showing bias of this type. First
Round Capital found that among its portfolio companies, startups
with female founders outperformed
those without by 63%. 
[2]The reason I began by saying that this technique would come as a
surprise to many people is that we so rarely see analyses of this
type.  I'm sure it will come as a surprise to First Round that they
performed one. I doubt anyone there realized that by limiting their
sample to their own portfolio, they were producing a study not of
startup trends but of their own biases when selecting companies.
If they'd understood the implications of the numbers they were
publishing, they wouldn't have presented them the way they did.I predict we'll see this technique used more in the future.  The
information needed to conduct such studies is increasingly available.
Data about who applies for things is usually closely guarded by the
organizations selecting them, but nowadays data about who gets
selected is often publicly available to anyone who takes the trouble
to aggregate it.
Notes[1]
This technique wouldn't work if the selection process looked
for different things from different types of applicants—for
example, if an employer hired men based on their ability but women
based on their appearance.[2]
As Paul Buchheit points out, First Round excluded their most 
successful investment, Uber, from the study.  And while it 
makes sense to exclude outliers from some types of studies, 
studies of returns from startup investing, which is all about 
hitting outliers, are not one of them.
Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading
drafts of this.
October 2015Here's a simple trick for getting more people to read what you
write: write in spoken language.Something comes over most people when they start writing. They write
in a different language than they'd use if they were talking to a
friend. The sentence structure and even the words are different.
No one uses "pen" as a verb in spoken English. You'd feel like an
idiot using "pen" instead of "write" in a conversation with a friend.The last straw for me was a sentence I read a couple days ago:

  The mercurial Spaniard himself declared: "After Altamira, all is
  decadence."

It's from Neil Oliver's A History of Ancient Britain. I feel bad
making an example of this book, because it's no worse than lots of
others.  But just imagine calling Picasso "the mercurial Spaniard" when
talking to a friend.  Even one
sentence of this would raise eyebrows in conversation.  And yet
people write whole books of it.Ok, so written and spoken language are different. Does that make
written language worse?If you want people to read and understand what you write, yes.
Written language is more complex, which makes it more work to read.
It's also more formal and distant, which gives the reader's attention
permission to drift.  But perhaps worst of all, the complex sentences
and fancy words give you, the writer, the false impression that
you're saying more than you actually are.You don't need complex sentences to express complex ideas.  When
specialists in some abstruse topic talk to one another about ideas
in their field, they don't use sentences any more complex than they
do when talking about what to have for lunch.  They use different
words, certainly.  But even those they use no more than necessary.
And in my experience, the harder the subject, the more informally
experts speak. Partly, I think, because they have less to prove,
and partly because the harder the ideas you're talking about, the
less you can afford to let language get in the way.Informal language is the athletic clothing of ideas.I'm not saying spoken language always works best. Poetry is as much
music as text, so you can say things you wouldn't say in conversation.
And there are a handful of writers who can get away with using fancy
language in prose. And then of course there are cases where writers
don't want to make it easy to understand what they're saying—in
corporate announcements of bad news, for example, or at the more
bogus end of the humanities.  But for nearly everyone else, spoken
language is better.It seems to be hard for most people to write in spoken language.
So perhaps the best solution is to write your first draft the way
you usually would, then afterward look at each sentence and ask "Is
this the way I'd say this if I were talking to a friend?" If it
isn't, imagine what you would say, and use that instead.  After a
while this filter will start to operate as you write. When you write
something you wouldn't say, you'll hear the clank as it hits the
page.Before I publish a new essay, I read it out loud and fix everything
that doesn't sound like conversation. I even fix bits that are
phonetically awkward; I don't know if that's necessary, but it
doesn't cost much.This trick may not always be enough.  I've seen writing so far
removed from spoken language that it couldn't be fixed sentence by
sentence.  For cases like that there's a more drastic solution.
After writing the first draft, try explaining to a friend what you
just wrote. Then replace the draft with what you said to your friend.People often tell me how much my essays sound like me talking.
The fact that this seems worthy of comment shows how rarely people
manage to write in spoken language.  Otherwise everyone's writing
would sound like them talking.If you simply manage to write in spoken language, you'll be ahead
of 95% of writers.  And it's so easy to do: just don't let a sentence
through unless it's the way you'd say it to a friend.
Thanks to Patrick Collison and Jessica Livingston for reading drafts of this.
October 2015When I talk to a startup that's been operating for more than 8 or
9 months, the first thing I want to know is almost always the same.
Assuming their expenses remain constant and their revenue growth
is what it's been over the last several months, do they make it to
profitability on the money they have left?  Or to put it more
dramatically, by default do they live or die?The startling thing is how often the founders themselves don't know.
Half the founders I talk to don't know whether they're default alive
or default dead.If you're among that number, Trevor Blackwell has made a handy
calculator you can use to find out.The reason I want to know first whether a startup is default alive
or default dead is that the rest of the conversation depends on the
answer.  If the company is default alive, we can talk about ambitious
new things they could do.  If it's default dead, we probably need
to talk about how to save it.  We know the current trajectory ends
badly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default
dead?  Mainly, I think, because they're not used to asking that.
It's not a question that makes sense to ask early on, any more than
it makes sense to ask a 3 year old how he plans to support
himself.  But as the company grows older the question switches from
meaningless to critical.  That kind of switch often takes people
by surprise.I propose the following solution: instead of starting to ask too
late whether you're default alive or default dead, start asking too
early.  It's hard to say precisely when the question switches
polarity.  But it's probably not that dangerous to start worrying
too early that you're default dead, whereas it's very dangerous to
start worrying too late.The reason is a phenomenon I wrote about earlier: the
fatal pinch.
The fatal pinch is default dead + slow growth + not enough
time to fix it.  And the way founders end up in it is by not realizing
that's where they're headed.There is another reason founders don't ask themselves whether they're
default alive or default dead: they assume it will be easy to raise
more money.  But that assumption is often false, and worse still, the
more you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking
of the future with vague optimism, explicitly separate the components.
Say "We're default dead, but we're counting on investors to save
us." Maybe as you say that it will set off the same alarms in your
head that it does in mine.  And if you set off the alarms sufficiently
early, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors
saving you.  As a rule their interest is a function of
growth.  If you have steep revenue growth, say over 6x a year, you
can start to count on investors being interested even if you're not
profitable.
[1]
But investors are so fickle that you can never
do more than start to count on it.  Sometimes something about your
business will spook investors even if your growth is great.  So no
matter how good your growth is, you can never safely treat fundraising
as more than a plan A. You should always have a plan B as well: you
should know (as in write down) precisely what you'll need to do to
survive if you can't raise more money, and precisely when you'll 
have to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the
sharp dichotomy many founders assume it to be.  In practice there
is surprisingly little connection between how much a startup spends
and how fast it grows.  When a startup grows fast, it's usually
because the product hits a nerve, in the sense of hitting some big
need straight on.  When a startup spends a lot, it's usually because
the product is expensive to develop or sell, or simply because
they're wasteful.If you're paying attention, you'll be asking at this point not just
how to avoid the fatal pinch, but how to avoid being default dead.
That one is easy: don't hire too fast.  Hiring too fast is by far
the biggest killer of startups that raise money.
[2]Founders tell themselves they need to hire in order to grow.  But
most err on the side of overestimating this need rather than
underestimating it.  Why?  Partly because there's so much work to
be done.  Naive founders think that if they can just hire enough
people, it somehow will be.  Partly because successful startups have
lots of employees, so it seems like that's what one does in order
to be successful.  In fact the large staffs of successful startups
are probably more the effect of growth than the cause.  And
partly because when founders have slow growth they don't want to
face what is usually the real reason: the product is not appealing
enough.Plus founders who've just raised money are often encouraged to
overhire by the VCs who funded them.  Kill-or-cure strategies are
optimal for VCs because they're protected by the portfolio effect.
VCs want to blow you up, in one sense of the phrase or the other.
But as a founder your incentives are different.  You want above all
to survive.
[3]Here's a common way startups die.  They make something moderately
appealing and have decent initial growth. They raise their first
round fairly easily because the founders seem smart and the idea
sounds plausible. But because the product is only moderately
appealing, growth is ok but not great.  The founders convince
themselves that hiring a bunch of people is the way to boost growth.
Their investors agree.  But (because the product is only moderately
appealing) the growth never comes.  Now they're rapidly running out
of runway.  They hope further investment will save them. But because
they have high expenses and slow growth, they're now unappealing
to investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem:
that the product is only moderately appealing.  Hiring people is
rarely the way to fix that.  More often than not it makes it harder.
At this early stage, the product needs to evolve more than to be
"built out," and that's usually easier with fewer people.
[4]Asking whether you're default alive or default dead may save you
from this.  Maybe the alarm bells it sets off will counteract the
forces that push you to overhire.  Instead you'll be compelled to
seek growth in other ways. For example, by doing
things that don't scale, or by redesigning the product in the
way only founders can.
And for many if not most startups, these paths to growth will be
the ones that actually work.Airbnb waited 4 months after raising money at the end of Y Combinator
before they hired their first employee.  In the meantime the founders
were terribly overworked.  But they were overworked evolving Airbnb
into the astonishingly successful organism it is now.Notes[1]
Steep usage growth will also interest investors.  Revenue
will ultimately be a constant multiple of usage, so x% usage growth
predicts x% revenue growth.  But in practice investors discount
merely predicted revenue, so if you're measuring usage you need a
higher growth rate to impress investors.[2]
Startups that don't raise money are saved from hiring too
fast because they can't afford to. But that doesn't mean you should
avoid raising money in order to avoid this problem, any more than
that total abstinence is the only way to avoid becoming an alcoholic.[3]
I would not be surprised if VCs' tendency to push founders
to overhire is not even in their own interest.  They don't know how
many of the companies that get killed by overspending might have
done well if they'd survived.  My guess is a significant number.[4]
After reading a draft, Sam Altman wrote:"I think you should make the hiring point more strongly.  I think
it's roughly correct to say that YC's most successful companies
have never been the fastest to hire, and one of the marks of a great
founder is being able to resist this urge."Paul Buchheit adds:"A related problem that I see a lot is premature scaling—founders
take a small business that isn't really working (bad unit economics,
typically) and then scale it up because they want impressive growth
numbers. This is similar to over-hiring in that it makes the business
much harder to fix once it's big, plus they are bleeding cash really
fast."
Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston,
and Geoff Ralston for reading drafts of this.
August 2015I recently got an email from a founder that helped me understand
something important: why it's safe for startup founders to be nice
people.I grew up with a cartoon idea of a very successful businessman (in
the cartoon it was always a man): a rapacious, cigar-smoking,
table-thumping guy in his fifties who wins by exercising power, and
isn't too fussy about how.  As I've written before, one of
the things that has surprised me most about startups is 
how few of
the most successful founders are like that.  Maybe successful people
in other industries are; I don't know; but not startup founders.
[1]I knew this empirically, but I never saw the math of why till I got
this founder's email.  In it he said he worried that he was
fundamentally soft-hearted and tended to give away too much for
free. He thought perhaps he needed "a little dose of sociopath-ness."I told him not to worry about it, because so long as he built
something good enough to spread by word of mouth, he'd have a
hyperlinear growth curve.  If he was bad at extracting money from
people, at worst this curve would be some constant multiple less
than 1 of what it might have been.  But a constant multiple of any
curve is exactly the same shape.  The numbers on the Y axis are
smaller, but the curve is just as steep, and when anything grows
at the rate of a successful startup, the Y axis will take care of
itself.Some examples will make this clear.  Suppose your company is making
$1000 a month now, and you've made something so great that it's
growing at 5% a week.  Two years from now, you'll be making about
$160k a month.Now suppose you're so un-rapacious that you only extract half as
much from your users as you could.  That means two years later
you'll be making $80k a month instead of $160k.  How far behind are
you? How long will it take to catch up with where you'd have been
if you were extracting every penny?  A mere 15 weeks.  After two
years, the un-rapacious founder is only 3.5 months behind the
rapacious one. 
[2]If you're going to optimize a number, the one to choose is your
growth rate. Suppose as before that you only extract half as much
from users as you could, but that you're able to grow 6% a week
instead of 5%.  Now how are you doing compared to the rapacious
founder after two years?  You're already ahead—$214k a month
versus $160k—and pulling away fast.  In another year you'll be
making $4.4 million a month to the rapacious founder's $2 million.Obviously one case where it would help to be rapacious is when
growth depends on that.  What makes startups different is that
usually it doesn't. Startups usually win by making something so
great that people recommend it to their friends.  And being rapacious
not only doesn't help you do that, but probably hurts.  
[3]The reason startup founders can safely be nice is that making great
things is compounded, and rapacity isn't.So if you're a founder, here's a deal you can make with yourself
that will both make you happy and make your company successful.
Tell yourself you can be as nice as you want, so long as you work
hard on your growth rate to compensate.  Most successful startups
make that tradeoff unconsciously. Maybe if you do it consciously
you'll do it even better.Notes[1]
Many think successful startup founders are driven by money.
In fact the secret weapon of the most successful founders is that
they aren't.  If they were, they'd have taken one of the acquisition
offers that every fast-growing startup gets on the way up.  What
drives the most successful founders is the same thing that drives
most people who make things: the company is their project.[2]
In fact since 2 ≈ 1.05 ^ 15, the un-rapacious founder is
always 15 weeks behind the rapacious one.[3]
The other reason it might help to be good at squeezing money
out of customers is that startups usually lose money at first, and
making more per customer makes it easier to get to profitability
before your initial funding runs out.  But while it is very common
for startups to die
from running through their initial funding and then being unable
to raise more, the underlying cause is usually slow growth or
excessive spending rather than insufficient effort to extract money
from existing customers.Thanks to Sam Altman, Harj Taggar, Jessica Livingston, and
Geoff Ralston for reading drafts of this, and to Randall Bennett
for being such a nice guy.
August 2015If you have a US startup called X and you don't have x.com, you
should probably change your name.The reason is not just that people can't find you.  For companies
with mobile apps, especially, having the right domain name is not
as critical as it used to be for getting users.  The problem with
not having the .com of your name is that it signals weakness.  Unless
you're so big that your reputation precedes you, a marginal domain
suggests you're a marginal company.  Whereas
(as Stripe shows)
having x.com signals strength even if it has no relation to what you
do.Even good founders can be in denial about this.  Their denial derives
from two very powerful forces: identity, and lack of imagination.X is what we are, founders think. There's no other name as good.
Both of which are false.You can fix the first by stepping back from the problem. Imagine
you'd called your company something else.  If you had, surely you'd
be just as attached to that name as you are to your current one.
The idea of switching to your current name would seem repellent.
[1]There's nothing intrinsically great about your current name.  Nearly
all your attachment to it comes from it being attached to you.
[2]The way to neutralize the second source of denial, your inability
to think of other potential names, is to acknowledge that you're
bad at naming.  Naming is a completely separate skill from those
you need to be a good founder.  You can be a great startup founder
but hopeless at thinking of names for your company.Once you acknowledge that, you stop believing there is nothing else
you could be called.  There are lots of other potential names that
are as good or better; you just can't think of them.How do you find them? One answer is the default way to solve
problems you're bad at: find someone else who can think of names.
But with company names there is another possible
approach.  It turns out almost any word or word pair that is not
an obviously bad name is a sufficiently good one, and the number
of such domains is so large that you can find plenty that are cheap
or even untaken.  So make a list and try to buy some.  That's what
Stripe 
did.  (Their search also turned up parse.com, which their
friends at Parse took.)The reason I know that naming companies is a distinct skill orthogonal
to the others you need in a startup is that I happen to have it.
Back when I was running YC and did more office hours with startups,
I would often help them find new names.  80% of the time we could
find at least one good name in a 20 minute office hour slot.Now when I do office hours I have to focus on more important
questions, like what the company is doing.  I tell them when they
need to change their name.  But I know the power of the forces that
have them in their grip, so I know most won't listen. 
[3]There are of course examples of startups that have succeeded without
having the .com of their name.  There are startups that have succeeded despite any
number of different mistakes.  But this mistake is less excusable
than most.  It's something that can be fixed in a couple days if
you have sufficient discipline to acknowledge the problem.100% of the top 20 YC companies by valuation have the .com of their
name. 94% of the top 50 do. But only 66% of companies in the current
batch have the .com of their name. Which suggests there are lessons
ahead for most of the rest, one way or another.
Notes[1]
Incidentally, this thought experiment works for
nationality and religion too.[2]
The liking you have for a name that has become part of your
identity manifests itself not directly, which would be easy to
discount, but as a collection of specious beliefs about its intrinsic
qualities.  (This too is true of nationality and religion as well.)[3]
Sometimes founders know it's a problem that they don't have
the .com of their name, but delusion strikes a step later in the belief that they'll
be able to buy it despite having no evidence it's for sale.  Don't
believe a domain is for sale unless the owner has already told you
an asking price.
Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston
for reading drafts of this.
February 2015One of the most valuable exercises you can try if you want to
understand startups is to look at the most successful companies and
explain why they were not as lame as they seemed when they first
launched.  Because they practically all seemed lame at first. Not
just small, lame.  Not just the first step up a big mountain.  More
like the first step into a swamp.A Basic interpreter for the Altair?  How could that ever grow into
a giant company?  People sleeping on airbeds in strangers' apartments?
A web site for college students to stalk one another?  A wimpy
little single-board computer for hobbyists that used a TV as a
monitor?  A new search engine, when there were already about 10,
and they were all trying to de-emphasize search?  These ideas didn't
just seem small.  They seemed wrong.  They were the kind of ideas
you could not merely ignore, but ridicule.Often the founders themselves didn't know why their ideas were
promising.  They were attracted to these ideas by instinct, because
they were living in the future and
they sensed that something was missing.  But they could not have
put into words exactly how their ugly ducklings were going to grow
into big, beautiful swans.Most people's first impulse when they hear about a lame-sounding
new startup idea is to make fun of it.  Even a lot of people who
should know better.When I encounter a startup with a lame-sounding idea, I ask "What
Microsoft is this the Altair Basic of?"  Now it's a puzzle, and the
burden is on me to solve it.  Sometimes I can't think of an answer,
especially when the idea is a made-up one.  But it's remarkable how
often there does turn out to be an answer.  Often it's one the
founders themselves hadn't seen yet.Intriguingly, there are sometimes multiple answers.  I talked to a
startup a few days ago that could grow into 3 distinct Microsofts.
They'd probably vary in size by orders of magnitude.  But you can
never predict how big a Microsoft is going to be, so in cases like
that I encourage founders to follow whichever path is most immediately
exciting to them.  Their instincts got them this far. Why stop now?
January 2015No one, VC or angel, has invested in more of the top startups than
Ron Conway.  He knows what happened in every deal in the Valley,
half the time because he arranged it.And yet he's a super nice guy.  In fact, nice is not the word.
Ronco is good. I know of zero instances in which he has behaved
badly.  It's hard even to imagine.When I first came to Silicon Valley I thought "How lucky that someone
so powerful is so benevolent."  But gradually I realized it wasn't
luck.  It was by being benevolent that Ronco became so powerful.
All the deals he gets to invest in come to him through referrals.
Google did. Facebook did. Twitter was a referral from Evan Williams
himself.  And the reason so many people refer deals to him is that
he's proven himself to be a good guy.Good does not mean being a pushover.  I would not want to face an
angry Ronco.  But if Ron's angry at you, it's because you did
something wrong.  Ron is so old school he's Old Testament.  He will
smite you in his just wrath, but there's no malice in it.In almost every domain there are advantages to seeming good.  It
makes people trust you.  But actually being good is an expensive
way to seem good.  To an amoral person it might seem to be overkill.In some fields it might be, but apparently not in the startup world.
Though plenty of investors are jerks, there is a clear trend among
them: the most successful investors are also the most upstanding. 
[1]It was not always this way.  I would not feel confident saying that
about investors twenty years ago.What changed?  The startup world became more transparent and more
unpredictable.  Both make it harder to seem good without actually
being good.It's obvious why transparency has that effect.  When an investor
maltreats a founder now, it gets out.  Maybe not all the way to the
press, but other founders hear about it, and that means that investor
starts to lose deals. 
[2]The effect of unpredictability is more subtle.  It increases the
work of being inconsistent.  If you're going to be two-faced, you
have to know who you should be nice to and who you can get away
with being nasty to.  In the startup world, things change so rapidly
that you can't tell.  The random college kid you talk to today might
in a couple years be the CEO of the hottest startup in the Valley.
If you can't tell who to be nice to, you have to be nice to everyone.
And probably the only people who can manage that are the people who
are genuinely good.In a sufficiently connected and unpredictable world, you can't seem
good without being good.As often happens, Ron discovered how to be the investor of the
future by accident.  He didn't foresee the future of startup
investing, realize it would pay to be upstanding, and force himself
to behave that way. It would feel unnatural to him to behave any
other way.  He was already 
living in the future.Fortunately that future is not limited to the startup world.  The
startup world is more transparent and unpredictable than most, but
almost everywhere the trend is in that direction.Notes[1]
I'm not saying that if you sort investors by benevolence
you've also sorted them by returns, but rather that if you do a
scatterplot with benevolence on the x axis and returns on the y,
you'd see a clear upward trend.[2]
Y Combinator in particular, because it aggregates data
from so many startups, has a pretty comprehensive view of
investor behavior.
Thanks to Sam Altman and Jessica Livingston for reading drafts of
this.
January 2015My father is a mathematician. For most of my childhood he worked
for Westinghouse, modelling nuclear reactors.He was one of those lucky people who know early on what they want to
do.  When you talk to him about his childhood, there's a clear
watershed at about age 12, when he "got interested in maths."  He
grew up in the small Welsh seacoast town of Pwllheli.  As we retraced
his walk to school on Google Street View, he said that it had been
nice growing up in the country."Didn't it get boring when you got to be about 15?" I asked."No," he said, "by then I was interested in maths."In another conversation he told me that what he really liked was
solving problems.  To me the exercises at the end of each chapter
in a math textbook represent work, or at best a way to reinforce
what you learned in that chapter.  To him the problems were the
reward.  The text of each chapter was just some advice about solving
them. He said that as soon as he got a new textbook he'd immediately
work out all the problems—to the slight annoyance of his teacher,
since the class was supposed to work through the book gradually.Few people know so early or so certainly what they want to work on.
But talking to my father reminded me of a heuristic the rest of us
can use. If something that seems like work to other people doesn't
seem like work to you, that's something you're well suited for.
For example, a lot of programmers I know, including me, actually
like debugging.  It's not something people tend to volunteer; one
likes it the way one likes popping zits. But you may have to like
debugging to like programming, considering the degree to which
programming consists of it.The stranger your tastes seem to other people, the stronger evidence
they probably are of what you should do. When I was in college I
used to write papers for my friends.  It was quite interesting to
write a paper for a class I wasn't taking.  Plus they were always
so relieved.It seemed curious that the same task could be painful to one person
and pleasant to another, but I didn't realize at the time what this
imbalance implied, because I wasn't looking for it.  I didn't realize
how hard it can be to decide what you should work on, and that you
sometimes have to figure it out from subtle clues, like a detective
solving a case in a mystery novel.  So I bet it would help a lot
of people to ask themselves about this explicitly. What seems like
work to other people that doesn't seem like work to you?
Thanks to Sam Altman, Trevor Blackwell, Jessica Livingston,
Robert Morris, and my father for reading drafts of this.
January 2015Corporate Development, aka corp dev, is the group within companies
that buys other companies. If you're talking to someone from corp
dev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to
sell your company right now and (b) you're sufficiently likely to
get an offer at an acceptable price.  In practice that means startups
should only talk to corp dev when they're either doing really well
or really badly.  If you're doing really badly, meaning the company
is about to die, you may as well talk to them, because you have
nothing to lose. And if you're doing really well, you can safely
talk to them, because you both know the price will have to be high,
and if they show the slightest sign of wasting your time, you'll
be confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young
companies that are growing fast, but haven't been doing it for long
enough to have grown big yet.  It's usually a mistake for a promising
company less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from
corp dev wants to meet, the founders tell themselves they should
at least find out what they want.  Besides, they don't want to
offend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying
you.  That's what the title "corp dev" means.   So before agreeing
to meet with someone from corp dev, ask yourselves, "Do we want to
sell the company right now?"  And if the answer is no, tell them
"Sorry, but we're focusing on growing the company."  They won't be
offended.  And certainly the founders of Big Company won't be
offended. If anything they'll think more highly of you.  You'll
remind them of themselves.  They didn't sell either; that's why
they're in a position now to buy other companies.
[1]Most founders who get contacted by corp dev already know what it
means.  And yet even when they know what corp dev does and know
they don't want to sell, they take the meeting.  Why do they do it?
The same mix of denial and wishful thinking that underlies most
mistakes founders make. It's flattering to talk to someone who wants
to buy you.  And who knows, maybe their offer will be surprisingly
high.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email,
sure, you might as well open it.  But that is not how conversations
with corp dev work.  If you get an offer at all, it will be at the
end of a long and unbelievably distracting process.  And if the
offer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And
conversations with corp dev are the worst sort of distraction,
because as well as consuming your attention they undermine your
morale.  One of the tricks to surviving a grueling process is not
to stop and think how tired you are.  Instead you get into a sort
of flow. 
[2]
Imagine what it would do to you if at mile 20 of a
marathon, someone ran up beside you and said "You must feel really
tired.  Would you like to stop and take a rest?"  Conversations
with corp dev are like that but worse, because the suggestion of
stopping gets combined in your mind with the imaginary high price
you think they'll offer.And then you're really in trouble.  If they can, corp dev people
like to turn the tables on you. They like to get you to the point
where you're trying to convince them to buy instead of them trying
to convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful
forces that can work on founders' minds, and attended by an experienced
professional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly
brutal. Corp dev people's whole job is to buy companies, and they
don't even get to choose which.  The only way their performance is
measured is by how cheaply they can buy you, and the more ambitious
ones will stop at nothing to achieve that. For example, they'll
almost always start with a lowball offer, just to see if you'll
take it. Even if you don't, a low initial offer will demoralize you
and make you easier to manipulate.And that is the most innocent of their tactics. Just wait till
you've agreed on a price and think you have a done deal, and then
they come back and say their boss has vetoed the deal and won't do
it for more than half the agreed upon price. Happens all the time.
If you think investors can behave badly, it's nothing compared to
what corp dev people can do.  Even corp dev people at companies
that are otherwise benevolent.  I remember once complaining to a
friend at Google about some nasty trick their corp dev people had
pulled on a YC startup. "What happened to Don't be Evil?" I asked.
"I don't think corp dev got the memo," he replied.
[3]The tactics you encounter in M&A conversations can be like nothing
you've experienced in the otherwise comparatively 
upstanding world
of Silicon Valley.  It's as if a chunk of genetic material from the
old-fashioned robber baron business world got incorporated into the
startup world.The simplest way to protect yourself is to use the trick that John
D. Rockefeller, whose grandfather was an alcoholic, used to protect
himself from becoming one.  He once told a Sunday school class

  Boys, do you know why I never became a drunkard?  Because I never
  took the first drink.

Do you want to sell your company right now?  Not eventually, right
now.  If not, just don't take the first meeting.  They won't be
offended.  And you in turn will be guaranteed to be spared one of
the worst experiences that can happen to a startup.If you do want to sell, there's another set of 
techniques
 for doing
that.  But the biggest mistake founders make in dealing with corp
dev is not doing a bad job of talking to them when they're ready
to, but talking to them before they are.  So if you remember only
the title of this essay, you already know most of what you need to
know about M&A in the first year.Notes[1]
I'm not saying you should never sell.  I'm saying you should
be clear in your own mind about whether you want to sell or not,
and not be led by manipulation or wishful thinking into trying to
sell earlier than you otherwise would have.[2]
In a startup, as in most competitive sports, the task at hand
almost does this for you; you're too busy to feel tired.  But when
you lose that protection, e.g. at the final whistle, the fatigue
hits you like a wave.  To talk to corp dev is to let yourself feel
it mid-game.[3]
To be fair, the apparent misdeeds of corp dev people are magnified
by the fact that they function as the face of a large organization
that often doesn't know its own mind.  Acquirers can be surprisingly
indecisive about acquisitions, and their flakiness is indistinguishable
from dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff
Ralston, and Qasar Younis for reading drafts of this.
December 2014American technology companies want the government to make immigration
easier because they say they can't find enough programmers in the
US.  Anti-immigration people say that instead of letting foreigners
take these jobs, we should train more Americans to be programmers.
Who's right?The technology companies are right. What the anti-immigration people
don't understand is that there is a huge variation in ability between
competent programmers and exceptional ones, and while you can train
people to be competent, you can't train them to be exceptional.
Exceptional programmers have an aptitude for and interest in
programming that is not merely the product of training.
[1]The US has less than 5% of the world's population.  Which means if
the qualities that make someone a great programmer are evenly
distributed, 95% of great programmers are born outside the US.The anti-immigration people have to invent some explanation to
account for all the effort technology companies have expended trying
to make immigration easier.  So they claim it's because they want
to drive down salaries.  But if you talk to startups, you find
practically every one over a certain size has gone through legal
contortions to get programmers into the US, where they then
paid them the same as they'd have paid an American.  Why would they
go to extra trouble to get programmers for the same price?  The
only explanation is that they're telling the truth: there are just
not enough great programmers to go around.
[2]I asked the CEO of a startup with about 70 programmers how many
more he'd hire if he could get all the great programmers he wanted.
He said "We'd hire 30 tomorrow morning."  And this is one of the
hot startups that always win recruiting battles. It's the same all
over Silicon Valley.  Startups are that constrained for talent.It would be great if more Americans were trained as programmers,
but no amount of training can flip a ratio as overwhelming as 95
to 5. Especially since programmers are being trained in other
countries too.  Barring some cataclysm, it will always be true that
most great programmers are born outside the US.  It will always be
true that most people who are great at anything are born outside
the US.
[3]Exceptional performance implies immigration.  A country with only
a few percent of the world's population will be exceptional in some
field only if there are a lot of immigrants working in it.But this whole discussion has taken something for granted: that if
we let more great programmers into the US, they'll want to come.
That's true now, and we don't realize how lucky we are that it is.
If we want to keep this option open, the best way to do it is to
take advantage of it: the more of the world's great programmers are
here, the more the rest will want to come here.And if we don't, the US could be seriously fucked. I realize that's
strong language, but the people dithering about this don't seem to
realize the power of the forces at work here.  Technology gives the
best programmers huge leverage.  The world market in programmers
seems to be becoming dramatically more liquid.  And since good
people like good colleagues, that means the best programmers could
collect in just a few hubs.  Maybe mostly in one hub.What if most of the great programmers collected in one hub, and it
wasn't here?  That scenario may seem unlikely now, but it won't be
if things change as much in the next 50 years as they did in the
last 50.We have the potential to ensure that the US remains a technology
superpower just by letting in a few thousand great programmers a
year.  What a colossal mistake it would be to let that opportunity
slip.  It could easily be the defining mistake this generation of
American politicians later become famous for.  And unlike other
potential mistakes on that scale, it costs nothing to fix.So please, get on with it.
Notes[1]
How much better is a great programmer than an ordinary one?
So much better that you can't even measure the difference directly.
A great programmer doesn't merely do the same work faster.  A great
programmer will invent things an ordinary programmer would never
even think of.  This doesn't mean a great programmer is infinitely
more valuable, because any invention has a finite market value.
But it's easy to imagine cases where a great programmer might invent
things worth 100x or even 1000x an average programmer's salary.[2]
There are a handful of consulting firms that rent out big
pools of foreign programmers they bring in on H1-B visas.  By all
means crack down on these.  It should be easy to write legislation
that distinguishes them, because they are so different from technology
companies.  But it is dishonest of the anti-immigration people to
claim that companies like Google and Facebook are driven by the
same motives.  An influx of inexpensive but mediocre programmers
is the last thing they'd want; it would destroy them.[3]
Though this essay talks about programmers, the group of people
we need to import is broader, ranging from designers to programmers
to electrical engineers.  The best one could do as a general term
might be "digital talent." It seemed better to make the argument a
little too narrow than to confuse everyone with a neologism.
Thanks to Sam Altman, John Collison, Patrick Collison, Jessica
Livingston, Geoff Ralston, Fred Wilson, and Qasar Younis for reading
drafts of this.
December 2014If the world were static, we could have monotonically increasing
confidence in our beliefs.  The more (and more varied) experience
a belief survived, the less likely it would be false.  Most people
implicitly believe something like this about their opinions.  And
they're justified in doing so with opinions about things that don't
change much, like human nature.  But you can't trust your opinions
in the same way about things that change, which could include
practically everything else.When experts are wrong, it's often because they're experts on an
earlier version of the world.Is it possible to avoid that?  Can you protect yourself against
obsolete beliefs?  To some extent, yes. I spent almost a decade
investing in early stage startups, and curiously enough protecting
yourself against obsolete beliefs is exactly what you have to do
to succeed as a startup investor.  Most really good startup ideas
look like bad ideas at first, and many of those look bad specifically
because some change in the world just switched them from bad to
good.  I spent a lot of time learning to recognize such ideas, and
the techniques I used may be applicable to ideas in general.The first step is to have an explicit belief in change.  People who
fall victim to a monotonically increasing confidence in their
opinions are implicitly concluding the world is static.  If you
consciously remind yourself it isn't, you start to look for change.Where should one look for it?  Beyond the moderately useful
generalization that human nature doesn't change much, the unfortunate
fact is that change is hard to predict.  This is largely a tautology
but worth remembering all the same: change that matters usually
comes from an unforeseen quarter.So I don't even try to predict it.  When I get asked in interviews
to predict the future, I always have to struggle to come up with
something plausible-sounding on the fly, like a student who hasn't
prepared for an exam.
[1]
But it's not out of laziness that I haven't
prepared.  It seems to me that beliefs about the future are so
rarely correct that they usually aren't worth the extra rigidity
they impose, and that the best strategy is simply to be aggressively
open-minded.  Instead of trying to point yourself in the right
direction, admit you have no idea what the right direction is, and
try instead to be super sensitive to the winds of change.It's ok to have working hypotheses, even though they may constrain
you a bit, because they also motivate you.  It's exciting to chase
things and exciting to try to guess answers.  But you have to be
disciplined about not letting your hypotheses harden into anything
more.
[2]I believe this passive m.o. works not just for evaluating new ideas
but also for having them.  The way to come up with new ideas is not
to try explicitly to, but to try to solve problems and simply not
discount weird hunches you have in the process.The winds of change originate in the unconscious minds of domain
experts.  If you're sufficiently expert in a field, any weird idea
or apparently irrelevant question that occurs to you is ipso facto
worth exploring. 
[3]
 Within Y Combinator, when an idea is described
as crazy, it's a compliment—in fact, on average probably a
higher compliment than when an idea is described as good.Startup investors have extraordinary incentives for correcting
obsolete beliefs.  If they can realize before other investors that
some apparently unpromising startup isn't, they can make a huge
amount of money.  But the incentives are more than just financial.
Investors' opinions are explicitly tested: startups come to them
and they have to say yes or no, and then, fairly quickly, they learn
whether they guessed right.  The investors who say no to a Google
(and there were several) will remember it for the rest of their
lives.Anyone who must in some sense bet on ideas rather than merely
commenting on them has similar incentives.  Which means anyone who
wants to have such incentives can, by turning their comments into
bets: if you write about a topic in some fairly durable and public
form, you'll find you worry much more about getting things right
than most people would in a casual conversation.
[4]Another trick I've found to protect myself against obsolete beliefs
is to focus initially on people rather than ideas. Though the nature
of future discoveries is hard to predict, I've found I can predict
quite well what sort of people will make them.  Good new ideas come
from earnest, energetic, independent-minded people.Betting on people over ideas saved me countless times as an investor.
We thought Airbnb was a bad idea, for example. But we could tell
the founders were earnest, energetic, and independent-minded.
(Indeed, almost pathologically so.)  So we suspended disbelief and
funded them.This too seems a technique that should be generally applicable.
Surround yourself with the sort of people new ideas come from.  If
you want to notice quickly when your beliefs become obsolete, you
can't do better than to be friends with the people whose discoveries
will make them so.It's hard enough already not to become the prisoner of your own
expertise, but it will only get harder, because change is accelerating.
That's not a recent trend; change has been accelerating since the
paleolithic era.  Ideas beget ideas.  I don't expect that to change.
But I could be wrong.
Notes[1]
My usual trick is to talk about aspects of the present that
most people haven't noticed yet.[2]
Especially if they become well enough known that people start
to identify them with you.  You have to be extra skeptical about
things you want to believe, and once a hypothesis starts to be
identified with you, it will almost certainly start to be in that
category.[3]
In practice "sufficiently expert" doesn't require one to be
recognized as an expert—which is a trailing indicator in any
case.  In many fields a year of focused work plus caring a lot would
be enough.[4]
Though they are public and persist indefinitely, comments on
e.g. forums and places like Twitter seem empirically to work like
casual conversation.  The threshold may be whether what you write
has a title.
Thanks to Sam Altman, Patrick Collison, and Robert Morris
for reading drafts of this.
December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least
two times, maybe three.  And yet if I had to write down everything
I remember from it, I doubt it would amount to much more than a
page.  Multiply this times several hundred, and I get an uneasy
feeling when I look at my bookshelves. What use is it to read all
these books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent
biography of Hilbert, I figured out if not the answer to this
question, at least something that made me feel better about it.
She writes:

  Hilbert had no patience with mathematical lectures which filled
  the students with facts but did not teach them how to frame a
  problem and solve it. He often used to tell them that "a perfect
  formulation of a problem is already half its solution."

That has always seemed to me an important point, and I was even
more convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A
combination of my own experience and other things I'd read.  None
of which I could at that moment remember!  And eventually I'd forget
that Hilbert had confirmed it too.  But my increased belief in the
importance of this idea would remain something I'd learned from
this book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if
you forget the experience or what you read, its effect on your model
of the world persists.  Your mind is like a compiled program you've
lost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle
is not what I remember from it, but my mental models of the crusades,
Venice, medieval culture, siege warfare, and so on.  Which doesn't
mean I couldn't have read more attentively, but at least the harvest
of reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But
it was a surprise to me and presumably would be to anyone else who
felt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about
forgetting, though.  There are specific implications.For example, reading and experience are usually "compiled" at the
time they happen, using the state of your brain at that time.  The
same book would get compiled differently at different points in
your life.  Which means it is very much worth reading important
books multiple times.  I always used to feel some misgivings about
rereading books.  I unconsciously lumped reading together with work
like carpentry, where having to do something again is a sign you
did it wrong the first time.  Whereas now the phrase "already read"
seems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology
will increasingly make it possible to relive our experiences.  When
people do that today it's usually to enjoy them again (e.g. when
looking at pictures of a trip) or to find the origin of some bug in
their compiled code (e.g. when Stephen Fry succeeded in remembering
the childhood trauma that prevented him from singing).  But as
technologies for recording and playing back your life improve, it
may become common for people to relive experiences without any goal
in mind, simply to learn from them again as one might when rereading
a book.Eventually we may be able not just to play back experiences but
also to index and even edit them. So although not knowing how you
know things may seem part of being human, it may not be.
Thanks to Sam Altman, Jessica Livingston, and Robert Morris for reading 
drafts of this.
